{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average/ Median / Mode\n",
    "\n",
    "Given a dataset we can calculate some charactheris of the dataset which act as an indicator of the dataset givining us the idea of the entire dataset.\n",
    "\n",
    "## Average\n",
    "\n",
    "Technically average is defined as follows.\n",
    "\n",
    "> Average is a number which when replaced with with other number in a dataset yields the same sum as the sum of the original dataset.\n",
    "\n",
    "Suppose we have following set $X=\\{1,2,3,4,5\\}$ of $n$ number where $n=5$ in this case, then the average or mean denoted by $\\bar{X}$, is a number where the following holds.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1 + 2 + 3 + 4 + 5 &= \\bar{X} + \\bar{X} +\\bar{X} +\\bar{X} +\\bar{X} \\\\\n",
    "\\sum_{x=i}^n x_i &= \\sum_{x=i}^n \\bar{X} \\\\\n",
    "\\sum_{x=i}^n x_i &= n * \\bar{X} \\\\\n",
    "\\bar{X} &= \\frac{\\sum x_i}{n} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus giving us the formula for the average. Which is the sum of number of elements divided by the number of elements.\n",
    "\n",
    "## Median\n",
    "\n",
    "Median of the dataset is the kind of center point or the center of gravity of the dataset. For the set $X=\\{1,2,3,4,5\\}$. The median would be $3$ as it is in between the set. If the number of element in the dataset are even we take the average of the midpoint.\n",
    "\n",
    "So more formally,\n",
    "\n",
    "$$\n",
    "\\text{Median Index} =\\begin{cases}\n",
    "\\frac{n+1}{2} & \\text{if n is odd} \\\\\n",
    "\\frac{\\frac{n}{2} + \\frac{n+1}{2}}{2} & \\text{if n is even} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Mode\n",
    "\n",
    "Mode is the most frequently occurring element in the dataset. For example in the set $X=\\{1,2,2,2,3,3,4\\}$. \"$2$\" is the mode as the has the most frequency of occurrence.\n",
    "\n",
    "# Variance and Standard deviation\n",
    "\n",
    "## Variance\n",
    "\n",
    "Average gives us the central tendency of the data. But it does not tells us how far the data is spread in the dataset. Consider following two sets. $X=\\{ 21, 22, 23, 24, 25 \\}$ and $Y=\\{1, 5, 9, 17, 83\\}$ both of the have the same average $\\bar{X}=23$ and $\\bar{Y}=23$. But $X$ is very less spread out then $Y$.\n",
    "\n",
    "To calculate how spread the data is from the mean we use variance denoted by $(\\delta^2)$. Variance is calculate by taking the sum of square of difference between each item and the mean divided by the total number of elements.\n",
    "\n",
    "$$\n",
    "\\delta^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{X})^2\n",
    "$$\n",
    "\n",
    "Thus for our example variance of $X$ is.\n",
    "\n",
    "$$\n",
    "\\delta^2 = \\frac{1}{5} \\left ( (21 - 23) ^2 + (22 - 23)^2 + (23 - 23)^2 + (24 - 23)^2 + (25 - 23)^2 \\right ) = 2\n",
    "$$\n",
    "\n",
    "Similarly of $Y$.\n",
    "\n",
    "$$\n",
    "\\delta^2 = \\frac{1}{5} \\left ( (1 - 23) ^2 + (5 - 23)^2 + (9 - 23)^2 + (17 - 23)^2 + (83 - 23)^2 \\right ) = 928\n",
    "$$\n",
    "\n",
    "As we can see $Y$ has very high variance as it is spread a lot.\n",
    "\n",
    "## Standard Deviation\n",
    "\n",
    "Variance is good for finding the spread, but in calculating the variance we square the terms thus converting unit to square units (cm to cm<sup>2</sup>). Thus we take square root of variance to get what is called is standard deviation (denoted by $\\delta$) which is in the same unit as that of original data.\n",
    "\n",
    "$$\n",
    "\\delta = \\sqrt{\\delta^2}\n",
    "$$\n",
    "\n",
    "If the data has a normal distribution. $68\\%$ of data will lie between $\\bar{X} \\pm \\delta$, $95\\%$ of data will lie between $\\bar{X} \\pm 2\\delta$ and $99.7\\%$ of data will lie between $\\bar{X} \\pm 3\\delta$.\n",
    "\n",
    "\n",
    "# Hypothesis Testing\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In statistics we generally have access to the sample statistics (Mean, Mode etc) and we make prediction of population parameters. But when predicting the prediction of population parameter how do we make sure that we are making the right prediction?.\n",
    "\n",
    "One way of proving this can be that we make sure that whenever someone else repeats the same experiment that we have done they will get the same result as ours. And they way we do it is by using *Hypothesis Testing*.\n",
    "\n",
    "Hypothesis testing in statistics is a way to test the results of a survey or experiment to see if you have meaningful results. We are basically testing whether our results are valid by figuring out the odds that our results have happened by chance. If our results may have happened by chance, the experiment won't be repeatable and so has little use.\n",
    "\n",
    "In hypothesis testing we follow a set of simple steps as follows.\n",
    "\n",
    "1. State out null hypothesis.\n",
    "2. State alternate hypothesis.\n",
    "3. Perform analysis on sample data.\n",
    "4. Either support or reject the null hypothesis based on result from step 3.\n",
    "\n",
    "### Null hypothesis\n",
    "\n",
    "A hypothesis is a statement we make about the data. For example the \"average hight of people is around $6$ feet\".\n",
    "\n",
    "To understand a null hypothesis we take following example. Suppose A researcher thinks that if knee surgery patients go to physical therapy twice a week (instead of 3 times), their recovery period will be longer. Average recovery times for knee surgery patients is 8.2 weeks. And we need to carry out experiment and verify this claim.\n",
    "\n",
    "Our first step to carry out the experiment we need to derive two hypothesis. A null hypothesis and a alternate hypothesis.\n",
    "\n",
    "A null hypothesi is the one which states the fact that we already know about the data. Or the universally expected facts. It is denoted by $H_0$.\n",
    "\n",
    "In our case our null hypothesis is $H_0 = 8.2$. Because it a know fact that our experiment is trying to challenge.\n",
    "\n",
    "### Alternate hypothesis\n",
    "\n",
    "Alternative hypothesis is the hypothesis that we are trying to proof. It is generally denoted by $H_a$ or $H_1$.\n",
    "\n",
    "In the example above the alternative hypothesis will be $H_a < 8.2$.\n",
    "\n",
    "### Performing analysis\n",
    "\n",
    "There are multiple analysis techniques are listed below which can be used depending upon the hypothesis and the data we are using in our experiment.\n",
    "\n",
    "**Variable Distribution Type Tests (Gaussian)**\n",
    "- Shapiro-Wilk Test\n",
    "- D’Agostino’s K^2 Test\n",
    "- Anderson-Darling Test\n",
    "\n",
    "**Variable Relationship Tests (correlation)**\n",
    "- Pearson’s Correlation Coefficient\n",
    "- Spearman’s Rank Correlation\n",
    "- Kendall’s Rank Correlation\n",
    "- Chi-Squared Test\n",
    "\n",
    "**Compare Sample Means (parametric)**\n",
    "- Student’s t-test\n",
    "- Paired Student’s t-test\n",
    "- Analysis of Variance Test (ANOVA)\n",
    "- Repeated Measures ANOVA Test\n",
    "\n",
    "**Compare Sample Means (nonparametric)**\n",
    "- Mann-Whitney U Test\n",
    "- Wilcoxon Signed-Rank Test\n",
    "- Kruskal-Wallis H Test\n",
    "- Friedman Test\n",
    "\n",
    "### Analyzing result of analysis\n",
    "\n",
    "Depending upon the type of analysis technique we use. We would get back either a **p-value** or the **critical values**\n",
    "\n",
    "#### P-values\n",
    "\n",
    "A statistical hypothesis test may return a value called **p** or the **p-value**. This is a quantity we can use to interpret or quantify the result of the test and either reject or fail to reject the null hypothesis. This is done by comparing the p-value to a threshold value chosen beforehand called the **significance level**.\n",
    "\n",
    "The significance level is often referred to by the Greek lower case letter alpha $(\\alpha)$.\n",
    "\n",
    "A common value used for alpha is $5\\%$ or $0.05$. A smaller $\\alpha$ value suggests a more robust interpretation of the null hypothesis, such as $1\\%$ or $0.1\\%$.\n",
    "\n",
    "For example, if we perform the experiment on the above example and we calculate a p-value of $0.07$. Then we could say that as the p-value is above the significant level we fail to reject the null hypothesis or we fail to accept the alternate hypothesis. So our null hypothesis still holds and thus the claim made by the researcher is falls.\n",
    "\n",
    "The significance level can be inverted by subtracting it from $1$ to give a confidence level of the hypothesis given the observed sample data.\n",
    "\n",
    "$$\n",
    "confidence = 1 - \\text{significance level}\n",
    "$$\n",
    "\n",
    "Continuing our example, we can state that the claim made by the researcher is falls by $93\\%$ confidence level.\n",
    "\n",
    "#### Critical Value\n",
    "\n",
    "In critical value also we choose a level of significance $(\\alpha)$. And we based on the critical value and level of significance we either reject or accept the null hypothesis.\n",
    "\n",
    "![critical value example](static/img/notes/mathematics/statistics/basic-statistics/critical_value_example.png)\n",
    "\n",
    "As seen from the above graph we have choose our significant value to be $\\alpha = 0.05$. Which in the distribution chosen evaluates to $1.645$ so any output of analysis which is create than $1.645$ is accepted and null hypothesis is rejected or we failed to reject null hypothesis and the $1.645$ is called the critical value.\n",
    "\n",
    "#### One-tailed vs two-tailed test.\n",
    "\n",
    "A test of a statistical hypothesis, where the region of rejection is on only one side of the sampling distribution, is called a one-tailed test. For example, suppose the null hypothesis states that the mean is less than or equal to 10. The alternative hypothesis would be that the mean is greater than 10. The region of rejection would consist of a range of numbers located on the right side of sampling distribution; that is, a set of numbers greater than 10.\n",
    "\n",
    "A test of a statistical hypothesis, where the region of rejection is on both sides of the sampling distribution, is called a two-tailed test. For example, suppose the null hypothesis states that the mean is equal to 10. The alternative hypothesis would be that the mean is less than 10 or greater than 10. The region of rejection would consist of a range of numbers located on both sides of sampling distribution; that is, the region of rejection would consist partly of numbers that were less than 10 and partly of numbers that were greater than 10.\n",
    "\n",
    "#### \"Rejecting\" vs \"Failure to Reject\"\n",
    "\n",
    "The p-value or the critical value is probabilistic.\n",
    "\n",
    "This means that when we interpret the result of a statistical test, we do not know what is true or false, only what is likely.\n",
    "\n",
    "Rejecting the null hypothesis means that there is sufficient statistical evidence that the null hypothesis does not look likely. Otherwise, it means that there is not sufficient statistical evidence to reject the null hypothesis.\n",
    "\n",
    "We may think about the statistical test in terms of the dichotomy of rejecting and accepting the null hypothesis. The danger is that if we say that we “accept” the null hypothesis, the language suggests that the null hypothesis is true. Instead, it is safer to say that we “fail to reject” the null hypothesis, as in, there is insufficient statistical evidence to reject it.\n",
    "\n",
    "### Types of Errors\n",
    "\n",
    "The interpretation of a statistical hypothesis test is probabilistic.\n",
    "\n",
    "That means that the evidence of the test may suggest an outcome and be mistaken. There are two types of error in hypothesis testing.\n",
    "\n",
    "#### Type one error (Rejecting true null hypothesis)\n",
    "\n",
    "When we reject the null hypothesis, although that hypothesis was true.\n",
    "\n",
    "#### Type two error (Accept false null hypothesis)\n",
    "\n",
    "When we accept the null hypothesis but it is false.\n",
    "\n",
    "## Chi Square Test of independence\n",
    "\n",
    "Chi square test of independence is an analysis method which is used to predict correlation between two categorical (Nominal) variables from a dataset.\n",
    "\n",
    "It is used to determine whether there is a significant association between two nominal variables.\n",
    "\n",
    "Chi square is usually denoted by $\\chi^2$ and pronounced as \"ki square\" from \"kite\".\n",
    "\n",
    "The idea of chi square independence test is simple. If there is no correlation between the variables (null hypothesis is true) then the frequency of the data must be normally distributed across the data. So we first calculate the frequency of the category in our sample dataset and then find the difference between it and expected frequency. Then we find the probability of getting a difference greater than one we found. If the probability is very low then it means that categories are not normally distributed and are not independent of each other.\n",
    "\n",
    "### When to use.\n",
    "\n",
    "Chi square is appropriate when the following conditions are met.\n",
    "\n",
    "- The sampling method is simple random sampling.\n",
    "- The variables under study are each categorical.\n",
    "- If sample data are displayed in a contingency table, the expected frequency count for each cell of the table is at least 5.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Suppose we have following dataset of $300$ people, which is $n=300$.\n",
    "\n",
    "![Chai Square 1](static/img/notes/mathematics/statistics/basic-statistics/chi-square-1.png)\n",
    "\n",
    "And we want to find out wether there is association/correlation between education and marriage status.\n",
    "\n",
    "#### Setup hypothesis and alpha\n",
    "\n",
    "In chi square test null hypothesis is always is that the two variable are independent of each other.\n",
    "\n",
    "$$\n",
    "H_0 = \\text{education and marriage are independent}\n",
    "$$\n",
    "\n",
    "And the alternative hypothesis is always that there is a correlation between them.\n",
    "\n",
    "$$\n",
    "H_a = \\text{education and marriage are not independent}\n",
    "$$\n",
    "\n",
    "We will use the $\\alpha = 0.05$ for this experiment.\n",
    "\n",
    "#### Calculate observed frequency.\n",
    "\n",
    "The first step we do is to calculate the observed frequency/count of the category from the dataset. And then create a table with column as categories of one variable and row as categories of other variable.\n",
    "\n",
    "![Chai Square 2](static/img/notes/mathematics/statistics/basic-statistics/chi-square-2.png)\n",
    "\n",
    "Let $i$ denote the row index and $j$ denoted the column index then we can denoted observed frequency as $O_{i,j}$\n",
    "\n",
    "#### Calculate the expected frequency.\n",
    "\n",
    "Next thing we do is calculate the expected frequency for each $(i, j)$. We do this by using the following formula.\n",
    "\n",
    "$$\n",
    "E_{i,j} = \\frac{T_i - T_j}{n}\n",
    "$$\n",
    "\n",
    "Where $E_{i, j}$ is the expected frequencey, $T_i$ is the row total and $T_j$ is the column total.\n",
    "\n",
    "![Chai Square 3](static/img/notes/mathematics/statistics/basic-statistics/chi-square-3.png)\n",
    "\n",
    "#### Residual\n",
    "\n",
    "Next we calculate the difference between observed frequency and the expected frequency.\n",
    "\n",
    "$$\n",
    "R_{i, j} = O_{i, j} - E_{i, j}\n",
    "$$\n",
    "\n",
    "Where $R_{i, j}$ is the residual.\n",
    "\n",
    "![Chai Square 4](static/img/notes/mathematics/statistics/basic-statistics/chi-square-4.png)\n",
    "\n",
    "#### Calculate Chi square $\\chi^2$\n",
    "\n",
    "Next we calculate the chi square by taking the sum of ratio of residual and expected frequencies.\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_{i,j} \\frac{R_{i,j}}{E_{i,j}} = \\sum_{i,j} \\frac{O_{i,j} - E_{i,j}}{E_{i,j}}\n",
    "$$\n",
    "\n",
    "For the above example $\\chi^2 = 23.57$.\n",
    "\n",
    "#### Degree of freedom\n",
    "\n",
    "Degrees of Freedom refers to the maximum number of logically independent values, which are values that have the freedom to vary, in the data sample.\n",
    "\n",
    "Consider a data sample consisting of five positive integers. The values could be any number with no known relationship between them. This data sample would, theoretically, have five degrees of freedom.\n",
    "\n",
    "Four of the numbers in the sample are {3, 8, 5, and 4} and the average of the entire data sample is revealed to be 6. This must mean that the fifth number has to be 10. It can be nothing else. It does not have the freedom to vary. So the Degrees of Freedom for this data sample is 4.\n",
    "\n",
    "The formula for Degrees of Freedom equals the size of the data sample minus one.\n",
    "\n",
    "$$\n",
    "Df = N -1\n",
    "$$\n",
    "\n",
    "For chi square tests, degrees of freedom is utilized to determine if a certain null hypothesis can be rejected based on the total number of variables and samples within the experiment. For example, when considering students and course choice, a sample size of 30 or 40 students is likely not large enough to generate significant data. Getting the same or similar results from a study using a sample size of 400 or 500 students is more valid.\n",
    "\n",
    "Below is a chart of degree of freedom for chi square distribution.\n",
    "\n",
    "![Chai Square 5](static/img/notes/mathematics/statistics/basic-statistics/chi-square-5.png)\n",
    "\n",
    "As we can see the more the dregree of freedom more the graph becomes close to normal distribution and thus more better prediction.\n",
    "\n",
    "So in chi square test the degree of freedom is given by\n",
    "\n",
    "$$\n",
    "Df = (i - 1)(j -1)\n",
    "$$\n",
    "\n",
    "So in our case of $5$ rows and $4$ columns the degree of freedom becomes $(5 -1) (4 -1) = 12$.\n",
    "\n",
    "#### Find p-value\n",
    "\n",
    "So using the graph of degree of freedom and our $\\chi^2$ we can find the p-value.\n",
    "\n",
    "In practice a chi probability table in used. [Link](https://www.medcalc.org/manual/chi-square-table.php)\n",
    "\n",
    "Using the table we found out that the value with $Df=12$ the probability of $\\chi^2 \\geq 23.57 \\approx 0.02$.\n",
    "\n",
    "We did this by looking at the row of $Df=12$ and finding a value which is greater than or close to $23.57$, which gave us the probability.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "We got the p-value of $0.02$ or $2\\%$. And our significant level was $0.05\\%$ that is $5\\%$. Thus our p-value is smaller than our significant level so we \"failed to reject\" the null hypothesis or in other words we accept the alternate hypothesis.\n",
    "\n",
    "The institutive meaning of this is that we have a very small chance for getting a sample which support our null hypothesis so we reject it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "note_info": {
   "description": "The basic of statistics.",
   "image": "static/img/notes/mathematics/statistics/basic-statistics/logo.jpeg",
   "slug": "basic-statistics",
   "title": "basic-statistics"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
