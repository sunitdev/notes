{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average/ Median / Mode\n",
    "\n",
    "Given a dataset we can calculate some charactheris of the dataset which act as an indicator of the dataset givining us the idea of the entire dataset.\n",
    "\n",
    "## Average\n",
    "\n",
    "Technically average is defined as follows.\n",
    "\n",
    "> Average is a number which when replaced with with other number in a dataset yields the same sum as the sum of the original dataset.\n",
    "\n",
    "Suppose we have following set $X=\\{1,2,3,4,5\\}$ of $n$ number where $n=5$ in this case, then the average or mean denoted by $\\bar{X}$, is a number where the following holds.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1 + 2 + 3 + 4 + 5 &= \\bar{X} + \\bar{X} +\\bar{X} +\\bar{X} +\\bar{X} \\\\\n",
    "\\sum_{x=i}^n x_i &= \\sum_{x=i}^n \\bar{X} \\\\\n",
    "\\sum_{x=i}^n x_i &= n * \\bar{X} \\\\\n",
    "\\bar{X} &= \\frac{\\sum x_i}{n} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus giving us the formula for the average. Which is the sum of number of elements divided by the number of elements.\n",
    "\n",
    "## Median\n",
    "\n",
    "Median of the dataset is the kind of center point or the center of gravity of the dataset. For the set $X=\\{1,2,3,4,5\\}$. The median would be $3$ as it is in between the set. If the number of element in the dataset are even we take the average of the midpoint.\n",
    "\n",
    "So more formally,\n",
    "\n",
    "$$\n",
    "\\text{Median Index} =\\begin{cases}\n",
    "\\frac{n+1}{2} & \\text{if n is odd} \\\\\n",
    "\\frac{\\frac{n}{2} + \\frac{n+1}{2}}{2} & \\text{if n is even} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Mode\n",
    "\n",
    "Mode is the most frequently occurring element in the dataset. For example in the set $X=\\{1,2,2,2,3,3,4\\}$. \"$2$\" is the mode as the has the most frequency of occurrence.\n",
    "\n",
    "# Variance and Standard deviation\n",
    "\n",
    "## Variance\n",
    "\n",
    "Average gives us the central tendency of the data. But it does not tells us how far the data is spread in the dataset. Consider following two sets. $X=\\{ 21, 22, 23, 24, 25 \\}$ and $Y=\\{1, 5, 9, 17, 83\\}$ both of the have the same average $\\bar{X}=23$ and $\\bar{Y}=23$. But $X$ is very less spread out then $Y$.\n",
    "\n",
    "To calculate how spread the data is from the mean we use variance denoted by $(\\delta^2)$. Variance is calculate by taking the sum of square of difference between each item and the mean divided by the total number of elements.\n",
    "\n",
    "$$\n",
    "\\delta^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{X})^2\n",
    "$$\n",
    "\n",
    "Thus for our example variance of $X$ is.\n",
    "\n",
    "$$\n",
    "\\delta^2 = \\frac{1}{5} \\left ( (21 - 23) ^2 + (22 - 23)^2 + (23 - 23)^2 + (24 - 23)^2 + (25 - 23)^2 \\right ) = 2\n",
    "$$\n",
    "\n",
    "Similarly of $Y$.\n",
    "\n",
    "$$\n",
    "\\delta^2 = \\frac{1}{5} \\left ( (1 - 23) ^2 + (5 - 23)^2 + (9 - 23)^2 + (17 - 23)^2 + (83 - 23)^2 \\right ) = 928\n",
    "$$\n",
    "\n",
    "As we can see $Y$ has very high variance as it is spread a lot.\n",
    "\n",
    "## Standard Deviation\n",
    "\n",
    "Variance is good for finding the spread, but in calculating the variance we square the terms thus converting unit to square units (cm to cm<sup>2</sup>). Thus we take square root of variance to get what is called is standard deviation (denoted by $\\delta$) which is in the same unit as that of original data.\n",
    "\n",
    "$$\n",
    "\\delta = \\sqrt{\\delta^2}\n",
    "$$\n",
    "\n",
    "If the data has a normal distribution. $68\\%$ of data will lie between $\\bar{X} \\pm \\delta$, $95\\%$ of data will lie between $\\bar{X} \\pm 2\\delta$ and $99.7\\%$ of data will lie between $\\bar{X} \\pm 3\\delta$.\n",
    "\n",
    "\n",
    "# Hypothesis Testing\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In statistics we generally have access to the sample statistics (Mean, Mode etc) and we make prediction of population parameters. But when predicting the prediction of population parameter how do we make sure that we are making the right prediction?.\n",
    "\n",
    "One way of proving this can be that we make sure that whenever someone else repeats the same experiment that we have done they will get the same result as ours. And they way we do it is by using *Hypothesis Testing*.\n",
    "\n",
    "Hypothesis testing in statistics is a way to test the results of a survey or experiment to see if you have meaningful results. We are basically testing whether our results are valid by figuring out the odds that our results have happened by chance. If our results may have happened by chance, the experiment won't be repeatable and so has little use.\n",
    "\n",
    "In hypothesis testing we follow a set of simple steps as follows.\n",
    "\n",
    "1. State out null hypothesis.\n",
    "2. State alternate hypothesis.\n",
    "3. Perform analysis on sample data assuming null hypothesis is true.\n",
    "4. Either support or reject the null hypothesis based on result from step 3.\n",
    "\n",
    "### Null hypothesis\n",
    "\n",
    "A hypothesis is a statement we make about the data. For example the \"average hight of people is around $6$ feet\".\n",
    "\n",
    "To understand a null hypothesis we take following example. Suppose A researcher thinks that if knee surgery patients go to physical therapy twice a week (instead of 3 times), their recovery period will be longer. Average recovery times for knee surgery patients is 8.2 weeks. And we need to carry out experiment and verify this claim.\n",
    "\n",
    "Our first step to carry out the experiment we need to derive two hypothesis. A null hypothesis and a alternate hypothesis.\n",
    "\n",
    "A null hypothesi is the one which states the fact that we already know about the data. Or the universally expected facts. It is denoted by $H_0$.\n",
    "\n",
    "In our case our null hypothesis is $H_0 = 8.2$. Because it a know fact that our experiment is trying to challenge.\n",
    "\n",
    "### Alternate hypothesis\n",
    "\n",
    "Alternative hypothesis is the hypothesis that we are trying to proof. It is generally denoted by $H_a$ or $H_1$.\n",
    "\n",
    "In the example above the alternative hypothesis will be $H_a < 8.2$.\n",
    "\n",
    "### Performing analysis\n",
    "\n",
    "After stating our null hypothesis and alternate hypothesis. We assume that the null hypothesis is true and we perform analysis on the data. Our goal of analysis is to find the probability of get the result that we got assuming the null hypothesis is true. If the probability (p-value) is small we reject our assumption of null hypothesis being correct is false and accept the alternate hypothesis and if probability is big then our assumption of null hypothesis is correct so we accept null hypothesis.\n",
    "\n",
    "There are multiple analysis techniques are listed below which can be used depending upon the hypothesis and the data we are using in our experiment.\n",
    "\n",
    "**Variable Distribution Type Tests (Gaussian)**\n",
    "- Shapiro-Wilk Test\n",
    "- D’Agostino’s K^2 Test\n",
    "- Anderson-Darling Test\n",
    "\n",
    "**Variable Relationship Tests (correlation)**\n",
    "- Pearson’s Correlation Coefficient\n",
    "- Spearman’s Rank Correlation\n",
    "- Kendall’s Rank Correlation\n",
    "- Chi-Squared Test\n",
    "\n",
    "**Compare Sample Means (parametric)**\n",
    "- Student’s t-test\n",
    "- Paired Student’s t-test\n",
    "- Analysis of Variance Test (ANOVA)\n",
    "- Repeated Measures ANOVA Test\n",
    "\n",
    "**Compare Sample Means (nonparametric)**\n",
    "- Mann-Whitney U Test\n",
    "- Wilcoxon Signed-Rank Test\n",
    "- Kruskal-Wallis H Test\n",
    "- Friedman Test\n",
    "\n",
    "### Analyzing result of analysis\n",
    "\n",
    "Depending upon the type of analysis technique we use. We would get back either a **p-value** or the **critical values**\n",
    "\n",
    "#### P-values\n",
    "\n",
    "A statistical hypothesis test may return a value called **p** or the **p-value**. This is a quantity we can use to interpret or quantify the result of the test and either reject or fail to reject the null hypothesis. This is done by comparing the p-value to a threshold value chosen beforehand called the **significance level**.\n",
    "\n",
    "The significance level is often referred to by the Greek lower case letter alpha $(\\alpha)$.\n",
    "\n",
    "A common value used for alpha is $5\\%$ or $0.05$. A smaller $\\alpha$ value suggests a more robust interpretation of the null hypothesis, such as $1\\%$ or $0.1\\%$.\n",
    "\n",
    "For example, if we perform the experiment on the above example and we calculate a p-value of $0.07$. Then we could say that as the p-value is above the significant level we fail to reject the null hypothesis or we fail to accept the alternate hypothesis. So our null hypothesis still holds and thus the claim made by the researcher is false.\n",
    "\n",
    "In other words if we select another random sample form the population assuming that null hypothesis is true, then p-value gives the probability that same outcome will appear again. Or to rephrase it, the the smaller the p-value, the more extreme or rare the observed data are, given the null hypothesis to be true. \n",
    "\n",
    "So we accept null hypothesis is p-value is greater than significant level and accept alternate hypothesis if p-value is smaller.\n",
    "\n",
    "The significance level can be inverted by subtracting it from $1$ to give a confidence level of the hypothesis given the observed sample data.\n",
    "\n",
    "$$\n",
    "confidence = 1 - \\text{significance level}\n",
    "$$\n",
    "\n",
    "Continuing our example, we can state that the claim made by the researcher is falls by $93\\%$ confidence level.\n",
    "\n",
    "#### Critical Value\n",
    "\n",
    "In critical value also we choose a level of significance $(\\alpha)$. And we based on the critical value and level of significance we either reject or accept the null hypothesis.\n",
    "\n",
    "![critical value example](static/img/notes/mathematics/statistics/basic-statistics/critical_value_example.png)\n",
    "\n",
    "As seen from the above graph we have choose our significant value to be $\\alpha = 0.05$. Which in the distribution chosen evaluates to $1.645$ so any output of analysis which is create than $1.645$ is accepted and null hypothesis is rejected or we failed to reject null hypothesis and the $1.645$ is called the critical value.\n",
    "\n",
    "#### One-tailed vs two-tailed test.\n",
    "\n",
    "A test of a statistical hypothesis, where the region of rejection is on only one side of the sampling distribution, is called a one-tailed test. For example, suppose the null hypothesis states that the mean is less than or equal to 10. The alternative hypothesis would be that the mean is greater than 10. The region of rejection would consist of a range of numbers located on the right side of sampling distribution; that is, a set of numbers greater than 10.\n",
    "\n",
    "A test of a statistical hypothesis, where the region of rejection is on both sides of the sampling distribution, is called a two-tailed test. For example, suppose the null hypothesis states that the mean is equal to 10. The alternative hypothesis would be that the mean is less than 10 or greater than 10. The region of rejection would consist of a range of numbers located on both sides of sampling distribution; that is, the region of rejection would consist partly of numbers that were less than 10 and partly of numbers that were greater than 10.\n",
    "\n",
    "#### \"Rejecting\" vs \"Failure to Reject\"\n",
    "\n",
    "The p-value or the critical value is probabilistic.\n",
    "\n",
    "This means that when we interpret the result of a statistical test, we do not know what is true or false, only what is likely.\n",
    "\n",
    "Rejecting the null hypothesis means that there is sufficient statistical evidence that the null hypothesis does not look likely. Otherwise, it means that there is not sufficient statistical evidence to reject the null hypothesis.\n",
    "\n",
    "We may think about the statistical test in terms of the dichotomy of rejecting and accepting the null hypothesis. The danger is that if we say that we “accept” the null hypothesis, the language suggests that the null hypothesis is true. Instead, it is safer to say that we “fail to reject” the null hypothesis, as in, there is insufficient statistical evidence to reject it.\n",
    "\n",
    "### Types of Errors\n",
    "\n",
    "The interpretation of a statistical hypothesis test is probabilistic.\n",
    "\n",
    "That means that the evidence of the test may suggest an outcome and be mistaken. There are two types of error in hypothesis testing.\n",
    "\n",
    "#### Type one error (Rejecting true null hypothesis)\n",
    "\n",
    "When we reject the null hypothesis, although that hypothesis was true.\n",
    "\n",
    "#### Type two error (Accept false null hypothesis)\n",
    "\n",
    "When we accept the null hypothesis but it is false.\n",
    "\n",
    "## Chi Square Test of independence\n",
    "\n",
    "Chi square test of independence is an analysis method which is used to predict correlation between two categorical (Nominal) variables from a dataset.\n",
    "\n",
    "It is used to determine whether there is a significant association between two nominal variables.\n",
    "\n",
    "Chi square is usually denoted by $\\chi^2$ and pronounced as \"ki square\" from \"kite\".\n",
    "\n",
    "The idea of chi square independence test is simple. If there is no correlation between the variables (null hypothesis is true) then the frequency of the data must be normally distributed across the data. So we first calculate the frequency of the category in our sample dataset and then find the difference between it and expected frequency. Then we find the probability of getting a difference greater than one we found. If the probability is very low then it means that categories are not normally distributed and are not independent of each other.\n",
    "\n",
    "### When to use.\n",
    "\n",
    "Chi square is appropriate when the following conditions are met.\n",
    "\n",
    "- The sampling method is simple random sampling.\n",
    "- The variables under study are each categorical.\n",
    "- If sample data are displayed in a contingency table, the expected frequency count for each cell of the table is at least 5.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Suppose we have following dataset of $300$ people, which is $n=300$.\n",
    "\n",
    "![Chai Square 1](static/img/notes/mathematics/statistics/basic-statistics/chi-square-1.png)\n",
    "\n",
    "And we want to find out wether there is association/correlation between education and marriage status.\n",
    "\n",
    "#### Setup hypothesis and alpha\n",
    "\n",
    "In chi square test null hypothesis is always is that the two variable are independent of each other.\n",
    "\n",
    "$$\n",
    "H_0 = \\text{education and marriage are independent}\n",
    "$$\n",
    "\n",
    "And the alternative hypothesis is always that there is a correlation between them.\n",
    "\n",
    "$$\n",
    "H_a = \\text{education and marriage are not independent}\n",
    "$$\n",
    "\n",
    "We will use the $\\alpha = 0.05$ for this experiment.\n",
    "\n",
    "#### Calculate observed frequency.\n",
    "\n",
    "The first step we do is to calculate the observed frequency/count of the category from the dataset. And then create a table with column as categories of one variable and row as categories of other variable.\n",
    "\n",
    "![Chai Square 2](static/img/notes/mathematics/statistics/basic-statistics/chi-square-2.png)\n",
    "\n",
    "Let $i$ denote the row index and $j$ denoted the column index then we can denoted observed frequency as $O_{i,j}$\n",
    "\n",
    "#### Calculate the expected frequency.\n",
    "\n",
    "Next thing we do is calculate the expected frequency for each $(i, j)$. We do this by using the following formula.\n",
    "\n",
    "$$\n",
    "E_{i,j} = \\frac{T_i \\times T_j}{n}\n",
    "$$\n",
    "\n",
    "Where $E_{i, j}$ is the expected frequency, $T_i$ is the row total and $T_j$ is the column total.\n",
    "\n",
    "\n",
    "#### Residual\n",
    "\n",
    "Next we calculate the residual using the following formula.\n",
    "\n",
    "$$\n",
    "R_{i, j} = \\frac{(O_{i, j} - E_{i, j})^2}{E_{i,j}}\n",
    "$$\n",
    "\n",
    "Where $R_{i, j}$ is the residual.\n",
    "\n",
    "\n",
    "#### Calculate Chi square $\\chi^2$\n",
    "\n",
    "Next we calculate the chi square by taking the sum of ratio of residual and expected frequencies.\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_{i,j} R_{i,j}\n",
    "$$\n",
    "\n",
    "#### Degree of freedom\n",
    "\n",
    "Degrees of Freedom refers to the maximum number of logically independent values, which are values that have the freedom to vary, in the data sample.\n",
    "\n",
    "Consider a data sample consisting of five positive integers. The values could be any number with no known relationship between them. This data sample would, theoretically, have five degrees of freedom.\n",
    "\n",
    "Four of the numbers in the sample are {3, 8, 5, and 4} and the average of the entire data sample is revealed to be 6. This must mean that the fifth number has to be 10. It can be nothing else. It does not have the freedom to vary. So the Degrees of Freedom for this data sample is 4.\n",
    "\n",
    "The formula for Degrees of Freedom equals the size of the data sample minus one.\n",
    "\n",
    "$$\n",
    "Df = N -1\n",
    "$$\n",
    "\n",
    "For chi square tests, degrees of freedom is utilized to determine if a certain null hypothesis can be rejected based on the total number of variables and samples within the experiment. For example, when considering students and course choice, a sample size of 30 or 40 students is likely not large enough to generate significant data. Getting the same or similar results from a study using a sample size of 400 or 500 students is more valid.\n",
    "\n",
    "Below is a chart of degree of freedom for chi square distribution.\n",
    "\n",
    "![Chai Square 5](static/img/notes/mathematics/statistics/basic-statistics/chi-square-5.png)\n",
    "\n",
    "As we can see the more the dregree of freedom more the graph becomes close to normal distribution and thus more better prediction.\n",
    "\n",
    "So in chi square test the degree of freedom is given by\n",
    "\n",
    "$$\n",
    "Df = (i - 1)(j -1)\n",
    "$$\n",
    "\n",
    "So in our case of $5$ rows and $4$ columns the degree of freedom becomes $(5 -1) (4 -1) = 12$.\n",
    "\n",
    "#### Find p-value\n",
    "\n",
    "So using the graph of degree of freedom and our $\\chi^2$ we can find the p-value.\n",
    "\n",
    "In practice a chi probability table in used. [Link](https://www.medcalc.org/manual/chi-square-table.php)\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "If We get a p-value less than our significant level $0.05$ that is $5\\%$, then we accept the alternate hypothesis.\n",
    "\n",
    "The institutive meaning of this is that we have a very small chance for getting a sample which support our null hypothesis so we reject it.\n",
    "\n",
    "## Students T test\n",
    "\n",
    "Student T test is a hypothesis test used to compare the mean of two sample, given that the sample size is small and the population follows a normal distribution. Essentially, a t-test allows to compare the average values of the two data sets and determine if they came from the same population. In other words it lets us know if those differences (measured in means/averages) could have happened by chance.\n",
    "\n",
    "The t-statistic was introduced in 1908 by \"William Sealy Gosset\", a chemist working for the Guinness brewery in Dublin, Ireland. \"Student\" was his pen name. Company policy at Guinness forbade its chemists from publishing their findings, so Gosset published his statistical work under the pseudonym \"Student\".\n",
    "\n",
    "After performing student t test we get a t-score which is a ratio between the difference between two groups and the difference within the groups.The larger the t-score, the more difference there is between groups. The smaller the t-score, the more similarity there is between groups. A t-score of 3 means that the groups are three times as different from each other as they are within each other.\n",
    "\n",
    "There are mainly three types of t tests.\n",
    "\n",
    "- One Sample T-Test (Sample mean vs Population Mean)\n",
    "- Unpaired two sample T-test (Mean of two independent samples)\n",
    "- Paired T-test (Mean of two related samples)\n",
    "\n",
    "### One Sample T-test\n",
    "\n",
    "The goal of one sample t test is to compare the mean of a random sample with a known mean of the population. This test is used the we know the mean of the population, but the standard deviation is unknown and we need to find out if a given sample belong to same population.\n",
    "\n",
    "First we formulate our null and alternate hypothesis. Our null hypothesis is that the mean of sample and population are same. And alternative hypothesis is that the means are different and the difference in numbers is due to chance.\n",
    "\n",
    "To evaluate our hypothesis we calculate the T-score using the following formula.\n",
    "\n",
    "Let $X$ sample represent a set of value with size $n$, with mean $\\bar{X}$ and standard deviation $S$. And let the mean of the population be $\\mu$.\n",
    "\n",
    "$$\n",
    "t= \\frac{\\bar{X} - \\mu}{S / \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "Then we find the degree of freedom using the formulae\n",
    "\n",
    "$$\n",
    "Df = 1 - n\n",
    "$$\n",
    "\n",
    "And assuming we have a significance level $\\alpha = 0.05 \\; (5\\%)$. Then we can use the [t-table](http://math.mit.edu/~vebrunel/Additional%20lecture%20notes/t%20(Student%27s)%20table.pdf) to find p-value.\n",
    "\n",
    "If the p-value is less than the significant level we reject the null hypothesis and accept the alternate hypothesis and if p-value is greater than we accept the null hypothesis.\n",
    "\n",
    "\n",
    "### Unpaired / Independent two sample t-test\n",
    "\n",
    "Independent (or unpaired two sample) t-test is used to compare the means of two unrelated groups of samples. As an example, we have a data of 100 individuals (50 women and 50 men). The question is to test whether the average weight of women is significantly different from that of men?\n",
    "\n",
    "In this case, we have two independents groups of samples and unpaired t-test can be used to test whether the means are different.\n",
    "\n",
    "We follow the same steps as we did for one sample t-test but only the formula for the t-score and degree of freedom changes.\n",
    "\n",
    "\n",
    "$$\n",
    "t= \\frac{\\bar{X_a} - \\bar{X_b}}{\\sqrt{\\frac{S_c^2}{n_a} + \\frac{S_c^2}{n_b}}}\n",
    "$$\n",
    "\n",
    "Where $\\bar{X_a}$ and $\\bar{X_b}$ are means of two independent samples $A$ and $B$ with size $n_a$ and $n_b$. And $S_c^2$ represents an estimator of the common variance of the two samples. Which is calculated by.\n",
    "\n",
    "$$\n",
    "S_c^2 = \\frac{\\sum (x - \\bar{X_a})^2 + \\sum (x - \\bar{X_b})^2}{n_a + n_b - 2}\n",
    "$$\n",
    "\n",
    "And the degree of freedom is given by.\n",
    "\n",
    "$$\n",
    "Df = n_a + n_b - 2\n",
    "$$\n",
    "\n",
    "\n",
    "### Paired sample t-test\n",
    "\n",
    "Paired Student’s t-test is used to compare the means of two related samples. That is when we have two values (pair of values) for the same samples.\n",
    "\n",
    "For example, 20 mice received a treatment X for 3 months. The question is to test whether the treatment X has an impact on the weight of the mice at the end of the 3 months treatment. The weight of the 20 mice has been measured before and after the treatment. This gives us 20 sets of values before treatment and 20 sets of values after treatment from measuring twice the weight of the same mice.\n",
    "\n",
    "In this case, paired t-test can be used as the two sets of values being compared are related. We have a pair of values for each mouse (one before and the other after treatment).\n",
    "\n",
    "Let $D^{(i)}$ be difference between each sample $X_a^{(i)}$ for first sample and $X_b^{(i)}$ or second sample, where $i$ is the row index. And let $n$ be the number of item in the sample. \n",
    "\n",
    "So the mean fo the difference can be calculated as\n",
    "\n",
    "$$\n",
    "\\bar{X_D} = \\frac{\\sum_i D}{n}\n",
    "$$\n",
    "\n",
    "And the standard of difference can be calculated as/\n",
    "\n",
    "$$\n",
    "S_D = \\sqrt{\\frac{1}{n} \\; \\sum_{i=1}^n (D^{(i)} - \\bar{X_D})}\n",
    "$$\n",
    "\n",
    "Then formula or t-score in paired t-test is given as.\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{X_D}}{S_D / \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "And the degree of freedom is given by.\n",
    "\n",
    "$$\n",
    "Df = n -1\n",
    "$$\n",
    "\n",
    "### Effect Size\n",
    "\n",
    "After performing t-test we can conclude if there is a significant difference between the mean of two sample. But we can also calculate how big is the difference between the means.\n",
    "\n",
    "There are multiple ways of calculating the effect size.\n",
    "\n",
    "#### Cohen's D\n",
    "\n",
    "Cohen's D formula is as follows.\n",
    "\n",
    "$$\n",
    "\\text{Cohen's D} = \\frac{\\bar{X_a} - \\bar{X_b}}{\\sqrt{SD_{pool}}}\n",
    "$$\n",
    "\n",
    "Where $\\bar{X_a}$ and $\\bar{X_b}$ are the mean of individual samples. And $SD_{pool}$ is the average of standard deviation of difference within the sample.\n",
    "\n",
    "$$\n",
    "SD_{pool} = \\frac{SD_a + SD_b}{2}\n",
    "$$\n",
    "\n",
    "The interpretation of $Cohen's D$ is as follows.\n",
    "\n",
    "If $D \\leq 0.2$ then effect is small, if $0.2 \\leq D \\leq 0.5$ then effect is medium and if $D \\geq 0.8$ the effect is large.\n",
    "\n",
    "#### R Squared\n",
    "\n",
    "$r^2$ is called as follows.\n",
    "\n",
    "$$\n",
    "r^2 = \\frac{t^2}{t^2 + Df}\n",
    "$$\n",
    "\n",
    "The interpretation of $r^2$  as follows.\n",
    "\n",
    "If $r^2 \\leq 0.01$ then effect is small, if $0.01 \\leq r^2 \\leq 0.09$ then effect is medium and if $r^2 \\geq 0.25$ the effect is large.\n",
    "\n",
    "\n",
    "## ANOVA\n",
    "\n",
    "Analysis of variance (ANOVA) is a statistical technique that is used to check if the means of two or more groups are significantly different from each other. ANOVA checks the impact of one or more factors by comparing the means of different samples. It does that by comparing the variance between the group and between the group, thus the name.\n",
    "\n",
    "ANOVA is similar to t-test but t-test can only check variance between two datasets (samples), whereas ANOVA works with multiple datasets.\n",
    "\n",
    "There are two type of ANOVA test based on the number of factors/group we are comparing variance with.\n",
    "\n",
    "- One-way ANOVA\n",
    "- Two-way ANOVA\n",
    "\n",
    "\n",
    "### One-way ANOVA\n",
    "\n",
    "To understand one ANOVA lets take an example.\n",
    "\n",
    "Suppose we have following data.\n",
    "\n",
    "| Group1 | Group2 | Group3 |\n",
    "|--------|--------|--------|\n",
    "| 3      | 5      | 5      |\n",
    "| 2      | 3      | 6      |\n",
    "| 1      | 4      | 7      |\n",
    "\n",
    "And we need to find out if there is a difference between the means of the three datasets.\n",
    "\n",
    "We will use $n$ to denote the number of datasets/rows and $m$ to denote the number of groups we have.\n",
    "\n",
    "#### Grand Mean.\n",
    "\n",
    "First we need to calculate means of individual dataset and a total means or also called as grand mean of the entire dataset.\n",
    "\n",
    "First we will calculate mean of individual dataset.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{X_{G1}} &= \\frac{3+2+1}{3} = 2 \\\\\n",
    "\\bar{X_{G2}} &= \\frac{5+3+4}{3} = 4 \\\\\n",
    "\\bar{X_{G3}} &= \\frac{5+6+7}{3} = 6 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now to calculate **grand mean** we can either take the sum of all dataset or we can take the average of all the individual means.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{X} &= \\frac{\\bar{X_{G1}} + \\bar{X_{G2}} + \\bar{X_{G3}}}{3} = 4 \\\\\n",
    "\\bar{X} &= \\frac{3+2+1+5+3+4+5+6+7}{9} = 4\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus or grand means $\\bar{X}$ is $4$.\n",
    "\n",
    "#### Total sum of square $(SST)$\n",
    "\n",
    "First we calculate the total variance between the data. We do this by taking sum of difference between each individual items and the grand mean.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "SST &= \\sum_{m} \\sum_{n} (x_{(m,n)} - \\bar{X})^2 \\\\\n",
    "&= (3-4)^2 + (2-4)^2 + (1-4)^2 + (5-4)^2 + (3-4)^2  \\\\\n",
    "&+ (4-4)^2 + (5-4)^2 + (6-4)^2 + (7-4)^2 \\\\\n",
    "SST &= 30\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So the total sum of square $(SST)$ is $30$ for this set of data. And the degree of freedom if $(m*n -1)$ because we are using $m*n$ number of items for calculating $SST$. In our case the degree of freedom for $SST$ will be $8$.\n",
    "\n",
    "#### Variance within the group $(SSW)$\n",
    "\n",
    "Now we calculate how far is data spread within from the individual means for each group. We do this by adding the variance of each individual group. In other words, for each group we take the sum of square of difference between its means and individual items and then take the sum of result for each group.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "SSW &= \\sum_m \\sum_n (x_{(m,n)} - \\bar{X}_{m}) \\\\\n",
    "&= (3-2)^2 + (2-2)^2 + (1-2)^2 \\\\\n",
    "&+ (5-4)^2 + (3-4)^2 + (4-4)^2 \\\\\n",
    "&+ (5-6)^2 + (6-6)^2 + (7-6)^2 \\\\\n",
    "SSW &= 6\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the degree of freedom is $m*(n-1)$ because we have $m$ groups and for each group the degree of freedom is $n-1$. For or example it will be $6$.\n",
    "\n",
    "To find the mean $SSW$ $(MSW)$ we divide the $SSW$ with the degree of freedom.\n",
    "\n",
    "$$\n",
    "MSW = \\frac{SSW}{m(n-1)} = \\frac{6}{6} = 1\n",
    "$$\n",
    "\n",
    "#### Variance between the groups $(SSB)$\n",
    "\n",
    "Then we calculate how far groups spread from each other. To do this we take the sum of weighted difference between th mean of each group and the grand mean.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "SSB &= \\sum_m n_m (\\bar{X_m} - \\bar{X}) \\\\\n",
    "&= 3 (2 - 4)^2 + 3(4-4)^2 + 3(6-4)^2 \\\\\n",
    "SSB &= 24\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The degree of freedom of $(SSB)$ is given by $m-1$ because we have $m$ number of unique groups. So we get degree of freedom of $2$ for our example.\n",
    "\n",
    "To find the means $SSB$ $(MSB)$ we do divide the $SSB$ with the degree of freedom.\n",
    "\n",
    "$$\n",
    "MSB = \\frac{SSB}{m-1} = \\frac{24}{2} = 12\n",
    "$$\n",
    "\n",
    "The image below given a good idea of variance between the group and variance within the group.\n",
    "\n",
    "![Variance Between VS Variance Within](static/img/notes/mathematics/statistics/basic-statistics/variance_between_vs_within_group.png).\n",
    "\n",
    "\n",
    "#### SST = SSB + SSW\n",
    "\n",
    "As we can see from example, there is a relation between $SST$ and $SSB$ and $SSW$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "SST &= SSB + SSW \\\\\n",
    "30 &= 24 + 6\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In other words the total variance in the data is sum of variance between the groups and the variance within the group.\n",
    "\n",
    "\n",
    "#### F-Value\n",
    "\n",
    "Now we calculate the f-value using the following ratio.\n",
    "\n",
    "$$\n",
    "\\text{f-value} = \\frac{MSB}{MSW} = \\frac{12}{1} = 12.\n",
    "$$\n",
    "\n",
    "This ratio is very intuitive. The numerator $MSB$ represents the variance between group, and denominator $MSW$ represents the variance within the group. If the numerator is higher than denominator (higher f-value) then there is bigger difference between the groups compare to variance within the group thus groups are different. And viz versa.\n",
    "\n",
    "#### p-value\n",
    "\n",
    "As always we define a significant level $(\\alpha)$ and for ANOVA we use the F distribution for finding the p-value. [Link](http://www.socr.ucla.edu/Applets.dir/F_Table.html).\n",
    "\n",
    "#### Interpretation.\n",
    "\n",
    "Interpretation is same as other hypothesis tests. If the p-value is smaller than the significant level we accept the alternative hypothesis and if p-value if greater then we accept the null hypothesis.\n",
    "\n",
    "The drawback of ANOVA is that it only tells us if there is any difference between the groups it does not tells there which groups have the difference.\n",
    "\n",
    "### Two-way ANOVA\n",
    "\n",
    "Two way anova is using one-way anova by making multiple combination of two datasets from a given dataset. For more info - [Link](https://stepupanalytics.com/two-way-anova-calculation-by-hand-analysis-of-variance/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "note_info": {
   "description": "The basic of statistics.",
   "image": "static/img/notes/mathematics/statistics/basic-statistics/logo.jpeg",
   "slug": "basic-statistics",
   "title": "basic-statistics"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
