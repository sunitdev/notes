{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Models and Axioms\n",
    "\n",
    "Many events cant be predicted with total certainty. The best we say is how **likely** there are to happen using the idea of probability. Probability can be thought as a mathematical framework for reasoning about uncertainty and developing approaches to inference problem.\n",
    "\n",
    "## Probabilities Model\n",
    "\n",
    "A probabilities model is a mathematical description of a random phenomenon. It is defined by its **sample space**, **events** within the sample space, and **probabilities** associated with each event.\n",
    "\n",
    "The sample space ( $S$ or $\\Omega$) for a probability model is the set of all possible outcomes. \n",
    "\n",
    "An event $A$ is a subset of the sample space $S$.\n",
    "\n",
    "A probability is a numerical value assigned to a given event $A$. The probability of an event is written $P(A)$, and describes the long-run relative frequency of the event. Probability is assigned using a probability law, which is defined according to the experiment.\n",
    "\n",
    "Every probabilistic model involves an underlying process, called the **experiment**. An experiment will produce exactly one and only one element of several possible outcomes.\n",
    "\n",
    "## Sample Space\n",
    "\n",
    "Sample space is a list (set) of all possible outcomes for an experiment. Sample is denoted by capital $S$ or $\\Omega$. Example of sample space for experiment of flipping a coin onces is ${H, T}$ which was two possible outcomes, either flip will result in heads ($H$) or tails $(T)$.\n",
    "\n",
    "### Rules for sample space\n",
    "\n",
    "Not all sets can be termed as sample space. For a set to be consider as sample space it must obey the following rules.\n",
    "\n",
    "#### Mutually Exclusive\n",
    "\n",
    "Every outcome (element) in the sample space set must be mutually exclusive. Which means when are experiment is perform the outcome must belong to one and only one element of the set, no two outcomes of the set can occur at a same time.\n",
    "\n",
    "#### Collectively Exhaustive\n",
    "\n",
    "Sample set must contains every possible outcome of an experiment. There should not be any outcome of an experiment which does not belongs to sample space.\n",
    "\n",
    "### Representation\n",
    "\n",
    "There are multiple ways to represent sample space.\n",
    "\n",
    "#### List of outcomes\n",
    "\n",
    "One way if representing sample space is to list all possible outcomes for the experiment.\n",
    "\n",
    "For example, suppose our experiment involves flipping a coin twice. Then our sample space will be as follows.\n",
    "\n",
    "$$\n",
    "S = \\Omega = \\{ HH, HT, TH, TT \\}\n",
    "$$\n",
    "\n",
    "#### Tree based sequential description\n",
    "\n",
    "Another way of representing sample space is to sequentially represent events in the sample space as branches of a tree and thus each branch represent a possible outcome of an experiment and the leaf of the tree represents the sample space for the experiment.\n",
    "\n",
    "For example, suppose our experiment involves flipping a coin twice. Then our sample space will be as follows.\n",
    "\n",
    "![Tree Sample Space](/static/img/notes/mathematics/intro-probability/sample_space_tree.png)\n",
    "\n",
    "### Discrete Sample Space\n",
    "\n",
    "A sample space is said to be discrete sample space if the sample space ($\\Omega$) is a finite set or countable infinity.\n",
    "\n",
    "Example of discrete sample space are flipping a coin, rolling a dice etc.\n",
    "\n",
    "### Continuous Sample Space\n",
    "\n",
    "Sample space is called as continuous if the sample space ($\\Omega$) contains uncountable infinite set of possible outcomes.\n",
    "\n",
    "Consider the example below, let see our experiment in to see the where the dart lands in the area marked by the rectangle at $x=1$ and $y=1$. \n",
    "\n",
    "![Continuous Sample Space](/static/img/notes/mathematics/intro-probability/continous_sample_space.png)\n",
    "\n",
    "Here our sample space is all the infinite set of rational points in the square reason. So our sample space is defined as follows,\n",
    "\n",
    "$$\n",
    "S = \\Omega = \\{ (x, y) | 0 \\leqslant x, y \\leqslant 1 \\}\n",
    "$$\n",
    "\n",
    "**Note:**\n",
    "\n",
    "Continuous sample spaces are interesting because we have a sample space with infinite outcomes in it. In the above example, the probability of hitting a point let say $(\\frac{1}{2}, \\frac{1}{3})$ with infinite precession is **zero**. But the collective probability is $1$, because the dart will definitely land somewhere in the 1 by 1 area. So to overcome this when we calculate probability we calculate probability of **event**, which is collection of many individual point (subset of sample space) instead of calculating probability of individual point. Even in discrete case we work with events, to be consistent.\n",
    "\n",
    "## Probability Axioms\n",
    "\n",
    "Every probabilistic model must follow these three rules called the axioms of probability.\n",
    "\n",
    "### Nonnegativity\n",
    "\n",
    "This axiom states that probability of an event cannot be negative.\n",
    "\n",
    "Let $A$ be any event,\n",
    "\n",
    "$$\n",
    "P(A) \\geqslant 0\n",
    "$$\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Normalization axiom states that the probability of sample space is always $1$\n",
    "\n",
    "$$\n",
    "P(\\Omega) = 1\n",
    "$$\n",
    "\n",
    "### Additivity\n",
    "\n",
    "Additivity axioms states that the probability of two event occurring together is sum of there individual probability.\n",
    "\n",
    "Let $A$ and $B$ be two events,\n",
    "\n",
    "$$\n",
    "\\text{If} A \\cap B \\ne \\phi \\; \\text{then} \\; P(A \\cup B) = P(A) + P(B)\n",
    "$$\n",
    "\n",
    "\n",
    "## Uniform Law\n",
    "\n",
    "### Discrete Case\n",
    "\n",
    "Let $X$ and $Y$ be two event of rolling a fair die. Then we can find out probabilities as follows.\n",
    "\n",
    "The probability of $X = 1 \\; \\text{and} \\; Y = 1$ or $X = 1 \\; \\text{and} \\; Y = 2$ is ,\n",
    "\n",
    "Le $A$ be the event where $X=1 \\; \\text{and} \\; Y=1$ and $B$ be the event where $X=1 \\; \\text{and} \\; Y=2$, then \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(A \\cup B) &= P(A) + P(B) \\tag{Additivity} \\\\\n",
    "&= \\frac{1}{36} + \\frac{1}{36} \\tag{Each outcome is equally likely} \\\\\n",
    "&= \\frac{2}{36} \\\\\n",
    "&= \\frac{1}{18} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The probability of $X = 1 \\; \\text{and} \\; Y = 1$ or $X = 1 \\; \\text{and} \\; Y = 2$ is $\\frac{1}{18}$.\n",
    "\n",
    "Another way of looking at the problem is listing down all the possible outcomes (Sample Space) and then counting the number of outcome we are interested in. \n",
    "\n",
    "So our sample space would be,\n",
    "\n",
    "$$\n",
    "S = \\Omega = \\{ (1,1), (1,2), (1,3), ....., (6,6) \\}\n",
    "$$\n",
    "\n",
    "In that we are interested in two outcomes, one is $(1, 1)$ and $(1,2)$. So the probability will be number of interested event upon the total number of possible events which is $\\frac{2}{36}$ or $\\frac{1}{18}$. This is called the **Discrete uniform law**. \n",
    "\n",
    "It states that, If all outcomes are equally likely and let $A$ be some event. Then\n",
    "\n",
    "$$\n",
    "P(A) = \\frac{\\text{Number of element of A}}{\\text{total number of element in sample space}}\n",
    "$$\n",
    "\n",
    "\n",
    "### Continuous Case\n",
    "\n",
    "For the case of continuous sample space the uniform law is simply the area under the event. \n",
    "\n",
    "For example.\n",
    "\n",
    "![Continuous Sample Space Example](/static/img/notes/mathematics/intro-probability/continous_sample_space_example.png)\n",
    "\n",
    "To calculate the probability of shaded reason where $X \\leqslant \\frac{1}{2}$ and $Y \\leqslant \\frac{1}{2}$. We calculate the area of the shaded reason, generally using integrals. But here since it forms a simple triangle we can use the area of triangle formula to calculate the area. So the probability of the event is,\n",
    "\n",
    "$$\n",
    "P(A) = \\frac{1}{2} * \\frac{1}{2} * \\frac{1}{2} = \\frac{1}{8}\n",
    "$$\n",
    "\n",
    "\n",
    "## Countable Additivity Axiom\n",
    "\n",
    "We can extend the additivity axiom to countable infinite set. As follows,\n",
    "\n",
    "Suppose we have countable infinite sample space $S$ and let $A_1, A_2, A_3, ....$ are disjoint events, then.\n",
    "\n",
    "$$\n",
    "P(A_1 \\cup A_2 \\cup A_3 \\cup ... ) = P(A_1) + P(A_2) + P(A_3) + ...\n",
    "$$\n",
    "\n",
    "For example.\n",
    "\n",
    "Let our experiment be flipping coin until we get a head. So our sample space becomes the number of flips (Countable Infinity).\n",
    "\n",
    "$$\n",
    "S = \\Omega = \\{1, 2, 3, ... \\}\n",
    "$$\n",
    "\n",
    "Now the probability if each event is a simple geometric series.\n",
    "\n",
    "$$\n",
    "P(1) = \\frac{1}{2}, P(2) = \\frac{1}{4}, P(3) = \\frac{1}{8}, ...\n",
    "$$\n",
    "\n",
    "Now lets say we want to find out probability of event where number of tosses are even. Thus\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P({2, 4, ...}) &= P(2) + P(4) + ... \\tag{Countable Additivity Axiom} \\\\\n",
    "&= \\frac{1}{2^2} + \\frac{1}{2^4} + ... \\\\\n",
    "&= \\frac{1}{3} \\tag{Sum of geometric progression}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus there is $\\frac{1}{3}$ or $33%$ probability there heads will turn up in even number of chances\n",
    "\n",
    "# Conditional Probability\n",
    "\n",
    "Conditional probability is used to calculate the probability of two dependent events. For example consider the following sample space.\n",
    "\n",
    "![Conditional Probability](/static/img/notes/mathematics/intro-probability/conditional_probability.png)\n",
    "\n",
    "As we can see there are two events A and B. With probabilities gives in the diagram. Finding probability of individual event is matter of adding the number for example,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A) &= \\frac{3}{6} + \\frac{2}{6} = \\frac{5}{6} \\\\\n",
    "P(B) &= \\frac{2}{6} + \\frac{1}{6} = \\frac{3}{6}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "But suppose we have told that event $B$ has occurred and now what is the probability of event $A$ occurring. Such probabilities where we know a certain event has happened and we want to find out the probability of another event based on our new believes we used conditional probability return as $P(A|B)$, read as *\"Probability of event A given that B occurred\"*.\n",
    "\n",
    "Intuitively, if we consider the above sample space. We know for sure that event $B$ has occurred so we can assume event $B$ as our new sample space and then finding out what is the probability of event $A$. As seen from the diagram, if event $B$ becomes our new sample space then $P(B) =  1$. But we must keep the ratio between the parts of event $B$ consistent. As we can say from the diagram event $B$ is divided into a ratio of $2:1$ so if $P(B) = 1$ and we keep the ratio consistent, then event $B$ must be divided into two parts of $\\frac{2}{3}$ and $\\frac{1}{3}$. So the probability of event $A$ occurring given that $B$ has occurred is $\\frac{2}{3}$.\n",
    "\n",
    "More formally, we can define conditional probability as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A|B) &= \\frac{P(A \\cup B)}{P(B)} \\tag{Assuming P(B) is not zero}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So in our above example.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A|B) &= \\frac{P(A \\cup B)}{P(B)} \\\\\n",
    "&= \\frac{\\frac{2}{6}}{\\frac{3}{6}} \\\\\n",
    "P(A |B) &= \\frac{2}{3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Multiplication Rule\n",
    "\n",
    "Now let's consider following example.\n",
    "\n",
    "![Multiplication Rule Example](/static/img/notes/mathematics/intro-probability/multiplication_rule_example.png)\n",
    "\n",
    "\n",
    "Suppose we have a radar installed in our premises which decided if an airplane is flying above the ground. Now consider two event.\n",
    "\n",
    "Event $A$ is airplane is flying above.\n",
    "\n",
    "Event $B$ is something is registered on the radar.\n",
    "\n",
    "And the probability of combination of each events is given in the diagram above.\n",
    "\n",
    "Now suppose we are to find following probabilities. 1) $P(A \\cap B)$: Probability of event is flying above and radar has registered something. 2) $P(B)$: Probability of something is registered on the radar both if actual plan is detected and false alarm. 3) $P(A|B)$: Probability of air plane is actually flying above the ground, given that something is registered on the radar.\n",
    "\n",
    "For first case, $P(A \\cap B)$ is probability of event $A$ occurring and probability of event $B$ occurring together. We can find it using the definition of condition probability which is $P(B|A) = \\frac{P(B \\cap A)}{P(A)} = \\frac{P(A \\cap B)}{P(A)}$ because $A \\cap B = B \\cap A$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A \\cap B) &= P(B|A) \\; P(A) \\\\\n",
    "&= 0.99 * 0.05 \\\\\n",
    "P(A \\cap B) &= 0.0495\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the second case $P(B)$ we have two events where $B$ will happen $P(B|A)$ or $P(B|A^c)$. So the total probability of $B$ is addition of two properties by additivity axiom.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(B) &= P(B|A) + P(B|A^c) \\\\\n",
    "&= 0.05 * 0.99 + 0.95 * 0.10 \\tag{calculated as same as first case} \\\\ \n",
    "P(B) &= 0.1445\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now for the last case $P(A|B)$ is start forward.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A|B) &= \\frac{P(A \\cap B)}{P(B)} \\\\\n",
    "&= \\frac{0.0495}{0.1445} \\\\\n",
    "P(A|B) &= 0.342\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can generalize this calculation as follows. Suppose we have probability model as such.\n",
    "\n",
    "![Multiplication Rule Example](/static/img/notes/mathematics/intro-probability/multiplication_rule_general.png)\n",
    "\n",
    "Then finding probability of any leaf node event is as simple as multiplying each each probability on the way from the root to the node. For example,\n",
    "\n",
    "$$\n",
    "P(A \\cap B \\cap C) = P(A) \\; P(B|A) \\; P(C |A \\cap B) \n",
    "$$\n",
    "\n",
    "And this is called the multiplication rule, we can even formally derive this rule.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A \\cap B \\cap C) &= P(A \\cap B) \\; P(C| A \\cap B) \\tag{conditional probability definition} \\\\\n",
    "P(A \\cap B \\cap C) &= P(A) \\; P(B|A) \\; P(C | A \\cap B) \\tag{conditional probability definition}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Total Probability Theorem\n",
    "\n",
    "Suppose we have following sample space with event $B$ divided into different parts of sample space, namely $A1, A2$ and $A3$. And we want to find the probability of event $B$.\n",
    "\n",
    "![Total Probability Theorem](/static/img/notes/mathematics/intro-probability/total_probability_example.png)\n",
    "\n",
    "We can compute the probability of event B by taking condition probability of each event with respect to $B$ and then taking weighted average for each probability. So that event (for example $A1$) which is more likely to occur get more probability. Thus total probability states the following.\n",
    "\n",
    "For every partition $A_i$ we do the following, (in the above case three)\n",
    "\n",
    "$$\n",
    "P(B) = P(A_1) \\; P(B|A_1) \\\\\n",
    "    + P(A_2) \\; P(B | A_2) \\\\\n",
    "    + P(A_3) \\; P(B | A_3)\n",
    "$$\n",
    "\n",
    "## Bayes Rule\n",
    "\n",
    "Our condition probability theorem and rules were based on initial believes. But using Bayes rule we can inference about an event. For example in the same diagram above if we know that event $B$ has occurred what is the probability that event $A_1$ has occurred, or in other words what is $P(A_1 | B)$\n",
    "\n",
    "We can derive it as follows\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A_1 \\mid B) &= \\frac{P(A_1 \\cap B)}{P(B)} \\tag{by definition} \\\\\n",
    "&= \\frac{P(A_1) P(B \\mid A_1)}{P(B)} \\tag{Multiplication Rule} \\\\\n",
    "&= \\frac{P(A_1) P(B \\mid A_1)}{\\sum_{i=N} P(A_i) \\; P(B \\mid A_i)} \\tag{total probability theorem} \\\\ \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "thus we can generalize this formula for any $A_i$ as follows,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A_i \\mid B) &= \\frac{P(A_i) P(B \\mid A_i)}{\\sum_{j=N} P(A_j) \\; P(B | A_j)} \\\\ \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "thus giving us the Bayes rule, which is foundation for inference probability.\n",
    "\n",
    "# Independence\n",
    "\n",
    "Two events are said to be independent if occurring of one event doesn't change our believes for the other event. Suppose there are two events $A$ and $B$ and we are told that event $B$ has occurred then it doesn't affects our initial believe about event $A$. More formally we can write.\n",
    "\n",
    "$$\n",
    "P(A|B) = P(A)\n",
    "$$\n",
    "\n",
    "thus occurring of event $B$ has no effect on event $A$. But the above definition is not used frequently because it is undefined where $P(B)=0$. So more accurate definition is.\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\; P(B) \\tag{multiplication rule}\n",
    "$$\n",
    "\n",
    "If the above identity holds for any events then the events are said to be independent. Following identity also work on a sequence of independent events.\n",
    "\n",
    "$$\n",
    "P(A_1 \\cap A_2 \\cap A_3 \\cap A_4) = P(A_1) \\; P(A_2) \\; P(A_3) \\; P(A_4)\n",
    "$$\n",
    "\n",
    "## Independence vs Disjoint events.\n",
    "\n",
    "\n",
    "![Independence vs Disjoint](/static/img/notes/mathematics/intro-probability/independence_vs_disjoint.png)\n",
    "\n",
    "Consider the above events, are these events independent? \n",
    "\n",
    "These events are disjoint but not independent because if we know that event $A$ has occurred then we are for sure that event $B$ has not occurred so both of the events are dependent on each other. Thus disjoint sets are not same as independent sets.\n",
    "\n",
    "## Conditional independence.\n",
    "\n",
    "Some events may become independent upon conditioning and some independent events might not remain independent after conditioning. Conditional independence is similar to that of normal independence with conditioning.\n",
    "\n",
    "$$\n",
    "P(A \\cap B | C) = P(A|C) \\; P(B|C)\n",
    "$$\n",
    "\n",
    "So to verify if a set of events is still independent after conditioning we should find the probability of each event after conditioning and the verify if the independence identity stills holds.\n",
    "\n",
    "## Independence vs Pairwise independence\n",
    "\n",
    "When more than two events are in consideration, then the event can be either all together independent of pair wise independent. For example,\n",
    "\n",
    "Let say we have an experiment of flipping a fair coin twice. And let there be three events as follows.\n",
    "\n",
    "**Event $A$**: First toss is head.\n",
    "\n",
    "**Event $B$**: Second toss is head.\n",
    "\n",
    "**Event $C$**: First and second toss gives same result.\n",
    "\n",
    "Now in this case our sample space is $\\{ HH, HT, TH, TT \\}$. So, here event $A$ and $B$ are pair wise independent, i.e knowing event $A$ has occurred doest changes our believe of event $B$. Similarly, event $A$ and $C$ are pair wise independent and event $B$ and $C$ are pair wise independent. But if we take all the three event together then these went are not independent because if we know that event $A$ and event $B$ has occurred then we are sure that event $C$ has occurred i.e $P(C) = 1$.\n",
    "\n",
    "We can prove that mathematically as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(C) &= \\frac{1}{2} \\\\\n",
    "P(C \\cap A) &= \\frac{1}{4} \\\\\n",
    "P(A \\cap B \\cap C)  &= \\frac{1}{4} \\\\\n",
    "P(C | A \\cap B) &= 1 \\\\\n",
    "\\therefore P(C) &\\neq P(C | A \\cap B)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus following independence identity doesn't hold true.\n",
    "\n",
    "$$\n",
    "P(A \\cap B \\cap C) \\neq P(A) \\; P(B) \\; P(C)\n",
    "$$\n",
    "\n",
    "Thus pair wise independence doesn't imply independent event, pair wise independent events must be independent together.\n",
    "\n",
    "\n",
    "# Discrete Random Variable\n",
    "\n",
    "Random variable are mathematical way to describe the outcome of experiment.\n",
    "\n",
    "![Random Variable Explained](static/img/notes/mathematics/intro-probability/random_variable_explain.png)\n",
    "\n",
    "Lets suppose we want to carry out experiment of selecting a random person from our sample space $(\\Omega)$ and record their height in feet. As shown in the figure, we pickup one random person from the sample space and then assign it to the number line $x$ based on it high. This assignment/conversion of a person (instance) to a real number is done by a function $H$ which take an outcome of an experiment and coverts it to a real number (height in our case). This function is called a **Random Variable**.\n",
    "\n",
    "Thus a random variable is a function which takes outcome of an experiment and converts it to a real number $(\\text{outcome} \\rightarrow \\mathbb{R})$.\n",
    "\n",
    "More formally we can define random variable as, a function from the sample space $\\Omega$ to the real number $\\mathbb{R}$.\n",
    "\n",
    "We generally used capital letter to denote random variable $(H)$ and small letter $(x)$ to denote the actual numerical value mapped by the random variable.\n",
    "\n",
    "An experiment can have multiple random variables associated with. For example we can pick a person and assign it to two real number line one for height and one for weight.\n",
    "\n",
    "A random variable can also take other random variable as input. For example in image above, $H$ can denote the height of the person in feet. And random variable $M$ can denote the height of person in meters which is $H * 0.3$.\n",
    "\n",
    "## Probability Mass Function\n",
    "\n",
    "As we know the definition of random variable we can get create a predict what will be value of the random variable using its probability mass function.\n",
    "\n",
    "Let's take an example. Suppose we have experiment of picking up to random numbers between $1$ and $4$. And Let $F$ be the random variable denoting the outcome of first experiment and $S$ be the random variable denoting the outcome of second experiment. And let $X$ be random variable which return minimum of first and second outcome. $X = min(F, S)$.\n",
    "\n",
    "Now suppose we want to find out what is probability that $X$ will be $2$. We can count all the possible outcome which make $X=2$ and then the sum of probabilities of each individual outcome will be equal to the probability of $X=2$. So the possible outcome are.\n",
    "\n",
    "$$\n",
    "(2,2), (2,3), (2,4), (3,2), (4,2)\n",
    "$$\n",
    "\n",
    "And each one has $\\frac{1}{16}$ probability or Probability Mass function (PMF) of $X=2$ written as $p_X(2)$ is.\n",
    "\n",
    "$$\n",
    "p_X(2) = \\frac{5}{16}\n",
    "$$\n",
    "\n",
    "Thus probability mass function give the probability of random variable taking a particular value. And the way to calculate the probability is to sum up all the outcomes from the sample space which make random value take the expected value.\n",
    "\n",
    "We can plot the PMF graph for X as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get sample space\n",
    "values_list = list(range(1, 5))\n",
    "sample_space = list(product(values_list, values_list))\n",
    "\n",
    "# Calculate PMF\n",
    "pmf = []\n",
    "for i in values_list:\n",
    "    pmf.append(len([item for item in sample_space if min(item[0], item[1]) == i]) / len(sample_space))\n",
    "\n",
    "\n",
    "# Plot bar chart\n",
    "plt.bar(values_list, pmf)\n",
    "plt.xticks(values_list, values_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the PMF of a random variable is equal to the probability of outcomes from sample space the following will be true. $\\sum_i p_X(x_i) = 1$, that is the sum of PMF for a random variable will always be equal to one.\n",
    "\n",
    "## Expectation\n",
    "\n",
    "One more property of random variable is called the Expected value of the random variable. Expected value of average of outcome of random variable if the experiment is carried out multiple times. For example suppose in our above example we picked up two random values multiple time then will be the average or the expected value of our random variable $X$.\n",
    "\n",
    "We can calculate that as follows. As the PMF gives use the probability of the random variable taking a particular value we can take a sum of the probability and the values of the random variable. Thus giving us the expected value of the random variable.\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_x x\\;p_X(x)\n",
    "$$\n",
    "\n",
    "So in our above example we can calculate the expected value of $X$ as follows.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[X] &= \\sum_x x\\;p_X(x) \\\\\n",
    "&= 1 \\; p_X(1) + 2 \\; p_X(1) + 3 \\; p_X(1) + 4 \\; p_X(1) \\\\\n",
    "& = 1 * \\frac{7}{16} + 2 * \\frac{5}{16} + 3 * \\frac{3}{16} + 4 * \\frac{1}{16} \\\\\n",
    "E[X] &= 1.87\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus if we repeat the above experiment multiple time we can expect the value of $X$ somewhat near $1.87$.\n",
    "\n",
    "### Properties of Expectation\n",
    "\n",
    "#### Expectation of Constant\n",
    "\n",
    "$$\n",
    "E[\\alpha] = \\alpha\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a constant.\n",
    "\n",
    "This property is intuitive. We can thing of $\\alpha$ as an experiment which always returns a constant then no matter how many times we carry out the experiment the average/expected value of the experiment will always be the same constant.\n",
    "\n",
    "#### Scalar Multiplication\n",
    "\n",
    "$$\n",
    "E[\\alpha X] = \\alpha E[X]\n",
    "$$\n",
    "\n",
    "Multiplying every number by same constant is same as multiplying the average with that constant.\n",
    "\n",
    "#### Linear function\n",
    "\n",
    "$$\n",
    "E[\\alpha X + \\beta] = \\alpha E[X] + \\beta\n",
    "$$\n",
    "\n",
    "Adding a constant value to every value of random variable is sample as adding the same value to the average of the random variable.\n",
    "\n",
    "#### Variance of a function of random variable\n",
    "\n",
    "$$\n",
    "E[g(X)] = \\sum_x g(x) p_X(x)\n",
    "$$\n",
    "\n",
    "Which is say the say as the function of some random variable is the probability of the outcomes that give rise to a particular value of $g(X)$, as $g(X)$ depends on $X$ which depends on the outcome.\n",
    "\n",
    "## Variance\n",
    "\n",
    "Variance $(\\sigma^2)$ lets us how far values of the random variables are spread from the expectation. We can calculate the same by taking the sum of square of difference between from the expected value and individual value of the random variable. Thus giving us the formula as follows.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "var(X) &= E[\\; [X - E[X]]^2 \\;] \\\\\n",
    "&= E[ \\; [X^2 - 2XE[X] + E[X]^2] \\;] \\\\\n",
    "&= E[X^2] - E[2XE[X]] + E[X]^2 \\\\\n",
    "&= E[X^2] - 2E[XE[X]] + E[X]^2 \\\\\n",
    "&= E[X^2] -2 E[X]E[X] + E[X]^2 \\\\\n",
    "&= E[X^2] - 2E[X]^2 + E[X]^2 \\\\\n",
    "var(X) &= E[X^2] - E[X]^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the variance of a random variable is given by.\n",
    "\n",
    "$$\n",
    "var(X) = \\sigma^2 = E[X^2] - E[X]^2\n",
    "$$\n",
    "\n",
    "### Properties of Variance.\n",
    "\n",
    "#### Variance if always positive.\n",
    "\n",
    "$$\n",
    "var(X) \\geq 0\n",
    "$$\n",
    "\n",
    "As variance is sum of positive number (squares). Variance will always be positive.\n",
    "\n",
    "#### Linear Equation\n",
    "\n",
    "$$\n",
    "var(\\alpha X + \\beta) = \\alpha^2 var(X)\n",
    "$$\n",
    "\n",
    "Variance measures the spread of the value from the expected value. Adding a constant $(\\beta)$ just shift the data to left or right on the number line but does not change the spread. And multiplying the variable by a scalar constant $(\\alpha)$, the constant becomes square when substituted inside the equation of variance.\n",
    "\n",
    "# Multiple Discrete Random Variable\n",
    "\n",
    "When dealing with multiple random variables in the same experiment we have to deal with same concepts of independence and conditional independence of probability with respect to random variable.\n",
    "\n",
    "## Conditional PMF\n",
    "\n",
    "Conditional PMF as similar to that of normal PMF but our initial believe/model changes based on the condition.\n",
    "\n",
    "Consider the following example.\n",
    "\n",
    "![Conditional PMF example](static/img/notes/mathematics/intro-probability/conditional_pmf_example.png)\n",
    "\n",
    "Suppose we have a random variable $X with the above PMF.\n",
    "\n",
    "And let suppose we have an event $A$ where $A = {X \\geq 2}$. And we are total that event $A$ has occurred. Then we denote the conditional PMF of $X$ with respected to $A$ as follows. $p_{X|A}(x)$.\n",
    "\n",
    "Similar to that of conditional probability the conditional PMF must sum up to $1$. Thus in our case the sum of conditional probability of all $x$ with respect to $A$ must sum up to $1$.\n",
    "\n",
    "$$\n",
    "\\sum_x p_{X|A}(x) = 1\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_{X|A}(2) &= \\frac{1}{3} \\\\\n",
    "p_{X|A}(3) &= \\frac{1}{3} \\\\\n",
    "p_{X|A}(4) &= \\frac{1}{3} \\\\\n",
    "\\sum_{x=2}^3 p_{X|A}(x) &= 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Conditional Expectation\n",
    "\n",
    "Similarly we can denote conditional expectation. We know the formula for expectation.\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_x p_X(x) * x\n",
    "$$\n",
    "\n",
    "Thus conditional expectation would be.\n",
    "\n",
    "$$\n",
    "E[X | A] = \\sum_x p_{X|A}(x) * x\n",
    "$$\n",
    "\n",
    "The only change is converting PMF to conditional PMF as the average/expectation will only scale based on the conditional model. So in our example.\n",
    "\n",
    "$$\n",
    "E[X | A] = \\frac{1}{3} * 2 + \\frac{1}{3} * 3 + \\frac{1}{3} * 4 = 3\n",
    "$$\n",
    "\n",
    "Which makes sense as $3$ is the average of all the values of random variable so we can expect $3$ be the final outcome if we repeat the same experiment multiple times.\n",
    "\n",
    "## Total Expectation Theorem\n",
    "\n",
    "We have seen earlier the *Total Probability Theorem* which is denoted as follows.\n",
    "\n",
    "$$\n",
    "P(B) = P(A_1)P(B | A_1) + P(A_2)P(B | A_2) + ... + P(A_n)P(B | A_n)\n",
    "$$\n",
    "\n",
    "Where $B$ is a combination of $A_n$ disjoint events.\n",
    "\n",
    "We can express the same formula in terms of PMF.\n",
    "\n",
    "$$\n",
    "p_X(x) = P(A_1)p_{X|A_1}(x) + ... + P(A_n)p_{X|A_n}(x)\n",
    "$$\n",
    "\n",
    "We are just replacing probability with PMF values of random variable. Now if we multiple $\\sum_x x$ on both side of the equation we get.\n",
    "\n",
    "$$\n",
    "\\sum_x x \\; p_X(x) = \\sum_x x \\; p_{X|A_1}(x) P(A_1) + ... + \\sum_x x \\; p_{X|A_n}(x) P(A_n)\n",
    "$$\n",
    "\n",
    "By definition $E[X] = \\sum_x x \\; p_X(x)$ and $E[X | A] = \\sum_x x \\; p_{X|A}(x)$ we get.\n",
    "\n",
    "$$\n",
    "E[X|A] = P(A_1)E[X | A_1] + ... + P(A_n)E[X|A_n]\n",
    "$$\n",
    "\n",
    "Thus giving us the *Total Expectation Theorem*.\n",
    "\n",
    "## Joint PMF\n",
    "\n",
    "Generally we need to find the association or relationship between two random variables. We do that using *Joint PMF*.\n",
    "\n",
    "Joint PMF is denoted as $p_{X, Y}(x, y)$ where each individual PMF is the sum of probability of $X$ and $Y$ event, that is $p_{X, Y}(x, y) = P(X = x \\; \\text{and} \\; Y = y )$.\n",
    "\n",
    "Example is shown below.\n",
    "\n",
    "![Joint PMF Example](static/img/notes/mathematics/intro-probability/joint_pmf_example.png)\n",
    "\n",
    "In the above example, if we need PMF of $X=2$ and $Y=3$ we can see the figure and let.\n",
    "\n",
    "$$\n",
    "p_{X, Y}(2, 3) = \\frac{4}{20}\n",
    "$$\n",
    "\n",
    "Joint PMF can be thought of as join probability so the sum of total probability must be one.\n",
    "\n",
    "$$\n",
    "\\sum_x \\sum_y p_{X, Y} (x, y) = 1\n",
    "$$\n",
    "\n",
    "\n",
    "## Marginal PMF\n",
    "\n",
    "We can use Joint PMF to find the PMF of individual value for a particular random variable. For example in the above diagram if we need to find the PMF of $X=3$ we can get add the value of the column where $X=3$.\n",
    "\n",
    "$$\n",
    "p_X(3) = \\frac{2}{20} + \\frac{1}{20} + \\frac{3}{20} = \\frac{5}{20}\n",
    "$$\n",
    "\n",
    "In other word what we did was in the formula of joint PMF we kept value of $X$ constant and then sum up all values of $Y$. We can formulate this as follows.\n",
    "\n",
    "$$\n",
    "p_X(x) = \\sum_y p_{X, Y}(x, y)\n",
    "$$\n",
    "\n",
    "Similarly for PMF of $Y$ we can write.\n",
    "\n",
    "$$\n",
    "p_Y(y) = \\sum_x p_{X, Y}(x, y)\n",
    "$$\n",
    "\n",
    "## Conditional joint PMF\n",
    "\n",
    "Conditional joint PMF is denoted by $p_{X|Y}(x|y)$. A conditional Joint PMF is defined as suppose we fixed value of one random variable we can calculate the PMF of the second random variable in our new model.\n",
    "\n",
    "For example suppose we know that $Y=2$ has occurred and we need to find the conditional PMF of $X$. Now our believe must stay between the second row as we know for sure that $Y=2$ has occurred. And the sum of probability must equal to $1$ so we can calculate the new PMF as follows, we must maintain the original proportion of the probability just scale them to sum up to $1$.\n",
    "\n",
    "$$\n",
    "\\sum_x p_{X|Y}(x | y) = 1  \\; \\text{or} \\; \\sum_y p_{Y|X}(y | x) = 1\n",
    "$$\n",
    "\n",
    "So our new probability wil become $\\frac{0}{5}, \\frac{1}{5}, \\frac{3}{5}, \\frac{1}{5}$.\n",
    "\n",
    "We can formulate this as follows.\n",
    "\n",
    "$$\n",
    "p_{X|Y}(x|y) = P(X = x | Y = y) = \\frac{p_{X, Y}(x, y)}{p_Y(y)}\n",
    "$$\n",
    "\n",
    "The above definition is same as that of conditional probability we are just scaling the existing probability based on conditional probability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "note_info": {
   "description": "Basic probability axioms and Random Variables",
   "image": "static/img/notes/mathematics/intro-probability/logo.jpg",
   "slug": "intro-probability",
   "title": "Introduction To Probability"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "344px",
    "width": "555px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
