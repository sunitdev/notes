{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Many events cant be predicted with total certainty. The best we can say is how ***likely*** they are to happen using the idea of probability. Probability can be thought as a mathematical framework for reasoning about uncertainty and developing approaches to inference problems.\n",
    "\n",
    "# Combinator Analysis\n",
    "\n",
    "Probability is all about likeness of an event to occur. Hence it would be useful to have an effective method for counting the number of ways a thing can occur. The mathematical theory of counting is formally knows as the combinatorial analysis.\n",
    "\n",
    "## The Basic Principle of Counting\n",
    "\n",
    "The basic principle of counting will be fundamental to all the theories in probability. Loosely put, it states that if one experiment can result in any of $m$ possible outcomes and if other experiment can result in any $n$ possible outcomes, then there are $mn$ possible outcomes of the two experiments.\n",
    "\n",
    "We can generalize the formula as such.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "If $r$ experiments that are to be performed such that the first one may result in any of $n_1$ possible outcomes; and if, for each of there $n_1$ possible outcomes, there are $n_2$ possible outcomes of the second experiment; and if, for each of the possible outcomes of the first two experiment, there are $n_3$ possible outcomes of the third experiment; then there is a total of $n_1 * n_2 ... n_r$ possible outcomes of the $r$ experiments.\n",
    "</div>\n",
    "\n",
    "\n",
    "## Permutations\n",
    "\n",
    "Permutation is number of ways we can arrange objects. Suppose we have three letters $a, b$ and $c$, how many different ordered arrangements of the letters are possible? In this case we can simply list it down and count. $abc, acb, bac, bca, cab$ and $cba$ as we can see there are $6$ possible arrangements. But we could have calculated it without listing it down. Since the first object in the permutation can be any of the $3$, second can be any of the $2$ and the third object is the remaining one. So the total possible permutations would have been $3 * 2 * 1 = 6$. Thus we can generalize this as follows.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Suppose we have $n$ objects. We can have following number of permutations for it. <br><br>\n",
    "$$\n",
    "n(n-1)(n-2)......3.2.1 = n!\n",
    "$$\n",
    "</div>\n",
    "\n",
    "Consider another case, how many permutation can be formed from the letter $PEPPER$?\n",
    "\n",
    "Since there are $6$ letter then total $6!$ combinations. But it is only true if we consider each $E$ and $P$ as separate letter. What if we consider all $P$ and all $E$ as single letter. The the permutation will be total permutation divided by the permutation possible for all $P's$ and all $E's$. Since there are three $P's$ possible permutation are $3!$ and for $E$ it is $2!$. So by basic principle of counting, the total permutation of $P's$ and $E's$ is $3! * 2!$ and the total permutation of all letter will be $\\frac{6!}{3!2!}$.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In general, for $n$ objects the total permutation are. <br><br>\n",
    "$$\n",
    "\\frac{n!}{n_1! * n_2! ... n_r!}\n",
    "$$\n",
    "<br>\n",
    "Where $n_1$ is alike and $n_2$ is alike and so on.\n",
    "</div>\n",
    "\n",
    "\n",
    "## Combinations\n",
    "\n",
    "We are often interested in determining the number of different groups of $r$ objects that could be formed form total of $n$ objects. For example, how many group of $3$ can we select from following $5$ items $A, B, C, D$ and $E$?. Since there are $5$ item the total permutation will be $5!$, however since we have group of $3$ same group of three items eg ABC will be counted $3!$ times (ABC, ACB, BCA, BAC, CAB, CBA). So to get the total number of group we different by number of repetition, thus giving $\\frac{5!}{3!}$ or $\\frac{5.4.3}{3.2.1}$\n",
    "\n",
    "In general, as $n(n-1)...(n-r+1)$ represents the number of different ways that a group of $r$ items could be selected from $n$ items when the order of selection is relevant, and each group of $r$ items will be counted $r!$ times in the count. So the final formula will be.\n",
    "\n",
    "$$\n",
    "\\frac{n(n-1)...(n-r+1)}{r!} = \\frac{n!}{(n-r)!r!}\n",
    "$$\n",
    "\n",
    "Combinations are generally denoted as $\\binom{n}{r}$ and read as \"n choose r\"\n",
    "\n",
    "## Multinomial Coefficients\n",
    "\n",
    "Suppose a set of $n$ distinct items is to be divided into $r$ distinct groups of respective sizes $n_1, n_2, ...,n_r$, where $\\sum_{i=1}^{r} n_i = n$. Then the number of possible group can be calculated as follows.\n",
    "\n",
    "We note that there are $\\binom{n}{n_1}$ possible choices for first group, $\\binom{n-n_1}{n_2}$ possible choices for second group, $\\binom{n-n_1-n_2}{n_3}$ for third group and so on. Thus by basic counting principle we can calculate total group count.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\binom{n}{n_1} \\binom{n-n_1}{n_2} ... \\binom{n-n_1-n_2 - ... n_{r-1}}{n_r} &=  \\\\\n",
    "&= \\frac{n!}{(n-n_1)!n_1!} \\frac{(n - n_1)!}{(n-n_1 -n_2)!n_2!} ... \\frac{(n - n_1 - ... - n_{r-1})!}{0!n_r!} \\\\\n",
    "&= \\frac{n!}{n_1!n_2!...n_r!}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "If $n_1+n_2+ .... +n_r = n$, we define $\\binom{n}{n_1, n_2, ..., n_r}$ by <br><br>\n",
    "$$\n",
    "\\binom{n}{n_1, n_2, ..., n_r} = \\frac{n!}{n_1! * n_2! ... n_r!}\n",
    "$$\n",
    "<br>\n",
    "Thus $\\binom{n}{n_1, n_2, ..., n_r}$ represents the number of possible divisions of $n$ distinct objects into $r$ distinct groups of respective sizes $n_1, n_2,..., n_r$\n",
    "</div>\n",
    "\n",
    "\n",
    "# Probability Axioms\n",
    "\n",
    "## Sample Space\n",
    "\n",
    "A probabilistic model is a mathematical description of an uncertain situation. Every probabilistic model must have two main ingredients.\n",
    "\n",
    "1) **Sample Space**: Which is the set of all possible outcomes of an experiment.\n",
    "\n",
    "2) The **Probability law**, which assigns the set $A$ of possible outcomes, a non-negative number $P(A)$ (called the probability of A) that encodes our knowledge or belief about the collective *likelihood* of the elements of $A$.\n",
    "\n",
    "Every probabilities model involves an underlying process, called the experiment, that will produce exactly one and only one element of several possible outcome. The set of all possible outcome is called the sample space of the experiment. For eg:\n",
    "\n",
    "1) Sample space for two coin toss (Discrete)\n",
    "\n",
    "$S = \\Omega = \\{HH, TT, HT, TH\\}$\n",
    "\n",
    "2) Sample space of dart landing in a particular area as show below. (Continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Get subplot\n",
    "fig = plt.figure()\n",
    "plot1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Plot Square\n",
    "X = np.arange(0, 1.1, 0.1)\n",
    "Y =  np.full(X.size, 1)\n",
    "plot1.plot(X, Y)\n",
    "\n",
    "X, Y = Y, X\n",
    "plot1.plot(X, Y)\n",
    "\n",
    "# Set figure size\n",
    "plot1.set_xlim(-1,2)\n",
    "plot1.set_ylim(-1, 2)\n",
    "\n",
    "# Show grid\n",
    "plot1.grid()\n",
    "plot1.axvline(x=0, color='black')\n",
    "plot1.axhline(y=0, color='black')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the sample space of dart landing any where between the shown square is.\n",
    "\n",
    "$$\n",
    "S = \\Omega = \\{(x, y) \\; | \\; 0 \\leqslant x, y \\leqslant 1\\}\n",
    "$$\n",
    "\n",
    "The second example is interesting because we have sample space with infinite points in it. So theoretically, the probability of hitting at a point $(X)$ in the sample space with infinite precession is **Zero**. But since we know that dart is always land in that area the total probability is $100%$ $( P(S) = P(\\Omega) =  1)$. Hence, to solve this paradox we measure probability of an **Event** which is collection of many individual points (subset) of sample space. So in continuous sample space we measure probability of event containing a set of points in a area $A(X)$ and in discrete sample, we still measure probability of an event but here event only contains a single element. \n",
    "\n",
    "### Representation\n",
    "\n",
    "Sample space can be represented in two ways.\n",
    "\n",
    "#### Listing down all elements.\n",
    "\n",
    "As seen above, the sample space can be represented by listing down all the elements in it. For example, sample space of two coin toss can be represented as.\n",
    "\n",
    "$$\n",
    "S = \\Omega = \\{HH, TT, HT, TH\\}\n",
    "$$\n",
    "\n",
    "#### Tree based sequential representation.\n",
    "\n",
    "Another way is to represent it as a tree. As shown below.\n",
    "\n",
    "![Sample Space Tree Representation](/static/img/notes/mathematics/intro-probability/sample_space_tree.png)\n",
    "\n",
    "\n",
    "Here the root is the start of the experiment and branches being all possible outcome at that stage.\n",
    "\n",
    "## Probability Axioms\n",
    "\n",
    "There are three main axioms in probability theory from which other axioms are derived.\n",
    "\n",
    "### Non Negativity : $P(A) \\geqslant 0$\n",
    "\n",
    "Fore every event $A$ its probability should be positive or Zero.\n",
    "\n",
    "### Normalization :  $P(S) = P(\\Omega) = 1$\n",
    "\n",
    "The probability of the entire sample space $(\\Omega)$ is equal to $1$.\n",
    "\n",
    "### Additivity : $P(A \\cup B) = P(A) + P(B)$\n",
    "\n",
    "If $A$ and $B$ are two distinct events then the probability of their union satisfies.\n",
    "\n",
    "$$\n",
    "P(A \\cup B) = P(A) + P(B)\n",
    "$$\n",
    "\n",
    "We can even prove additivity for $N$ numbers of disjoint events. Such as follows.\n",
    "\n",
    "Let $A, B$ and $C$ be disjoint events, then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(A \\cup B \\cup C) &= P(A \\cup B) + P(C) \\tag{using additivity} \\\\\n",
    "P(A \\cup B \\cup C) &= P(A) + P(B) + P(C) \\tag{using additivity on first term}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "So more generally, if the sample space has an infinite numbers of elements and $A_1, A_2, A_3, ...$ is sequence of all disjoint events then <br><br>\n",
    "$$\n",
    "P(A_1 \\cup A_2 \\cup A_3 \\cup ...) = P(A_1) + P(A_2) + P(A_3) + ...\n",
    "$$\n",
    "</div>\n",
    "\n",
    "## Discrete uniform law.\n",
    "\n",
    "Lets take an example. For an experiment of three coin toss, what is the probability of exactly 2 heads occurring.\n",
    "\n",
    "Here our sample space will be.\n",
    "\n",
    "$$\n",
    "S = \\Omega = \\{ HHH, HHT, HTH, HTT, THH, THT, TTH, TTT \\}\n",
    "$$\n",
    "\n",
    "And let $A$ be the event of exactly $2$ heads.\n",
    "\n",
    "$$\n",
    "A = \\{ HHT, HTH, THH \\}\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(A) &= P(\\{ HHT, HTH, THH \\}) \\\\\n",
    "&= P(\\{HHT\\}) + P(\\{ HTH \\}) + P(\\{THH\\}) \\tag{using additivity} \\\\\n",
    "&= \\frac{1}{8} + \\frac{1}{8} + \\frac{1}{8} \\\\\n",
    "P(A) &= \\frac{3}{8}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here since every outcome had equal probability or *likelihood* we could add their individual probability.\n",
    "\n",
    "We can generalize this as discrete uniform law as follows, if a sample space consists of $n$ possible outcomes which are equally likely (ie. all single element have the same probability), then the probability of any event $A$ is given by\n",
    "\n",
    "$$\n",
    "P(A) = \\frac{\\text{number of elements of A}}{n}\n",
    "$$\n",
    "\n",
    "## Properties of Probability laws\n",
    "\n",
    "Following relations can be derived using the axioms.\n",
    "\n",
    "1. If $A \\subset B$, then $P(A) \\leqslant P(B)$.\n",
    "2. $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$\n",
    "3. $P(A \\cup B \\cup C) = P(A) + P(A^c \\cap B) + P(A^c \\cap B^c \\cap C^c)$\n",
    "4. De Morgan law\n",
    "$$\n",
    "\\left ( \\bigcup\\limits_{i=1}^{n} A_{i}^{c} \\right )^c = \\bigcap\\limits_{i=1}^{n} \\left ( A_{i}^{c} \\right )^c\n",
    "$$\n",
    "and vice versa.\n",
    "\n",
    "# Conditional Probability\n",
    "\n",
    "Conditional probability provides us with a way to reason about the outcome of an experiment based on **partial information**.\n",
    "\n",
    "For example, if in an experiment of two successive rolls of die, we are told that sum of the two rolls is $9$. How likely it is that the first roll was a $6$?\n",
    "\n",
    "In more precise terms, given an experiment, a corresponding sample space and a probability law, suppose that we know that the outcome is within some given event B. We wish to quantify the likelihood that the outcome also belongs to some other given event $A$. We thus seek to construct a new probability law that takes into account the available knowledge. A probability law that for any event $A$ specifies the conditional probability of $A$ given $B$ is denoted by $P(A|B)$.\n",
    "\n",
    "![Conditional Probability Example](/static/img/notes/mathematics/intro-probability/conditional_probability.png)\n",
    "\n",
    "In such situations we are given that event $B$ has occurred. So now event $B$ becomes are new sample space and thus $P(A|B)$ can be calculated by,\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "**NOTE:** $P(A|B)$ is undefined when $P(B) = 0$.\n",
    "\n",
    "## Conditional Probability Tools\n",
    "\n",
    "### Multiplication Rule\n",
    "\n",
    "Let's suppose we have three events $A, B$ and $C$ in sequence, then the we can draw a tree representation something like this.\n",
    "\n",
    "![Conditional Multiplication Tree](/static/img/notes/mathematics/intro-probability/conditional_multiplication_rule.png)\n",
    "\n",
    "Then the multiplication rule states that.\n",
    "\n",
    "$$\n",
    "P(A \\cap B \\cap C) = P(A) * P(B | A) * P(C | A \\cap B)\n",
    "$$\n",
    "\n",
    "we get this formula by following the leaf through the root and multiplying every probability along the way.\n",
    "\n",
    "More generally we can write\n",
    "\n",
    "$$\n",
    "P \\left ( \\bigcap\\limits_{i=1}^{n} A_i \\right ) = P(A_1)*P(A_2 | A_1)*P(A_3 | A_1 \\cap A_2) ...\n",
    "$$\n",
    "\n",
    "or,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(A | B) &= \\frac{P(A \\cap B)}{P(B)} \\\\\n",
    "P(A | B) * P(B) &= P(A \\cap B)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus giving us the multiplication rule. $P(A \\cap B) = P(A | B) * P(B)$\n",
    "\n",
    "### Total Probability Theorem\n",
    "\n",
    "![Conditional Total Probability](/static/img/notes/mathematics/intro-probability/conditional_total_probability.png)\n",
    "\n",
    "Here the event $B$ is divided into partition by subsets of sample space $A_1, A_2$ and $A_3$. So the total probability of $B$ can be computed by.\n",
    "\n",
    "$$\n",
    "P(B) = P(A_1) * P(B|A_1) + P(A_2) * P(B|A_2) + P(A_3) * P(B | A_3)\n",
    "$$\n",
    "\n",
    "where $P(B|A_1)$ is same as $P(P_1)$, $P(B|A_2)$ is same as $P(P_2)$ and $P(B|A_3)$ is same as $P(P_3)$.\n",
    "\n",
    "The probability of event $B$ is calculated by taking weighted average of each individual piece of $B$ depending upon the probability of each partition.\n",
    "\n",
    "We can generalize this into total probability theorem as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(B) &= P(A_1 \\cap B) + P(A_2 \\cap B)+ ... + P(A_n \\cap B) \\\\\n",
    "&= P(A_1) * P(B|A_1) + P(A_2) * P(B|A_2) + ... + P(A_n) * P(B | A_n)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Bayes Rule\n",
    "\n",
    "In the same diagram as that of total probability theorem, if we know the $P(B)$ and we know $P(B|A_i)$ and we need to compute $P(A_i|B)$ then,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(A_i | B) &= \\frac{P(A_i \\cap B)}{P(B)} \\tag{By definition} \\\\\n",
    "&= \\frac{P(A_i) P(B | A_i)}{P(B)} \\tag{Multiplication rule} \\\\\n",
    "P(A_i | B) &= \\frac{P(A_i) P(B | A_i)}{\\sum_j P(A_j)P(B | A_j)} \\tag{Total Probability Theorem}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus Baye's rule states that,\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Let $A_1, A_2, ..., A_n$ be disjoint events which partition event $B$, then <br><br>\n",
    "$$\n",
    "P(A_i | B) = \\frac{P(A_i) P(B | A_i)}{P(A_1) P(B|A_1) + ... + P(A_n) P(B|A_n)}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "## Independence\n",
    "\n",
    "Two events are said to be independent when occurrence of one event does not affects or tell anything more about occurrence of other events. More conceptually we can say.\n",
    "\n",
    "$$ \n",
    "P(A | B) = P(A) \n",
    "$$\n",
    "\n",
    "Which means knowing event $B$ has occurred doesn't change our believes of event $A$, such events are called **independent events**.\n",
    "\n",
    "By above definition following equality  holds,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(A | B) &= \\frac{P(A \\cap B)}{P(B)} \\\\\n",
    "P(A) &= \\frac{P(A \\cap B)}{P(B)} \\\\\n",
    "\\text{therefore,} \\\\\n",
    "P(A \\cap B) &= P(A)*P(B)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Or more generally,\n",
    "\n",
    "$$\n",
    "P(A_1...A_n) = \\sum\\limits_{i=1}^{n} P(A_i)\n",
    "$$\n",
    "\n",
    "We adopt this relation to test independence because it can be used even when $P(B)=0$. And, the symmetry of this relation also implies that independence is a symmetric property, if $A$ is independent to $B$, then $B$ is also independent to $A$.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "Independence is not similar to disjoint of a set. For example.\n",
    "\n",
    "![Disjoint Sets](/static/img/notes/mathematics/intro-probability/disjoint_sets.png)\n",
    "\n",
    "Here two events $A$ and $B$ are not independent because if somehow we know that event $A$ has occurred $(P(A) = 1)$ then event $B$ can't occur $P(B) = 0$. Thus both are dependent on each other, or more precisely, the probability or likelihood is dependent on each other\n",
    "\n",
    "### Conditional Independence\n",
    "\n",
    "We can modify the definition of independence to incorporate conditioning as follows.\n",
    "\n",
    "$$\n",
    "P(A \\cap B | C) = P(A|C) * P(B|C)\n",
    "$$\n",
    "\n",
    "where $A$ and $B$ are independent events, conditioned on $C$.\n",
    "\n",
    "**Note:** Independence does not imply conditional independence, both are different.\n",
    "\n",
    "# Discrete Random Variables\n",
    "\n",
    "In many probabilities models the outcome of every experiment is some numerical value. Even if the outcome is not numeric we could assign some numeric value to the outcome. So given an experiment when we assign a numerical value to the outcome of the experiment, the numerical value is called the **value of random variable** and the function which takes outcome of an experiment and produces a numerical value if called the **random variable**. Diagram below will illustrate the concept of random variable.\n",
    "\n",
    "![Random Variable Concept](/static/img/notes/mathematics/intro-probability/random_variable_concept.png)\n",
    "\n",
    "For example, suppose that our experiment consists of tossing $3$ fair coin. If we let $Y$ denote the number of heads that appear, the $Y$ is a random variable taking on one of the values $0, 1, 2$ and $3$ with respective probabilities.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P\\{Y = 0\\} &= P\\{(TTT)\\} = \\frac{1}{8} \\\\\n",
    "P\\{Y = 1\\} &= P\\{(TTH, THT, HTT)\\} = \\frac{3}{8} \\\\\n",
    "P\\{Y = 2\\} &= P\\{(THH, HTH, HHT)\\} = \\frac{3}{8} \\\\\n",
    "P\\{Y = 3\\} &= P\\{(HHH)\\} = \\frac{1}{8} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since $Y$ must take on one of the value $0$ through $3$, we must have\n",
    "\n",
    "$$\n",
    "1 = P \\left ( \\bigcup\\limits_{i=0}^{3} \\{ Y = i \\} \\right ) = \\sum\\limits_{i=0}^{3} P\\{ Y = i \\}\n",
    "$$\n",
    "\n",
    "which, of course, is in accord with the preceding probabilities.\n",
    "\n",
    "Random variables are also of two types discrete random variable which return a discrete value for each outcome and continuous random variable which is a continuous function.\n",
    "\n",
    "## Probability Mass Function\n",
    "\n",
    "For a discrete random variable $X$ the probability mass function $(PMF)$ of $X$ is denoted by $P_X$. For example if $x$ is any possible value of $X$, then the $PMF$ is the probability of the event $\\{ X = x\\}$, consisting of all the outcome that give rise to a value of $X$ equal to $x$.\n",
    "\n",
    "For example suppose we have following sample space $\\Omega$ and some random variable $X$.\n",
    "\n",
    "![Discrete PMF](/static/img/notes/mathematics/intro-probability/discrete_pmf.png)\n",
    "\n",
    "Here the $P_X(1)=\\frac{5}{9}$, because in original sample space we have event $A$ which produce $X=1$ and $P(A)=\\frac{5}{9}$. Thus $PMF$ of $X$ or $P_{X}=\\frac{5}{9}$. So we can define.\n",
    "\n",
    "$$\n",
    "P_X(x)=P(\\{X = x\\})\n",
    "$$\n",
    "\n",
    "So, to calculate the $PMF$ of a random variable $X$ we do the following.\n",
    "\n",
    "For each possible value $x$ of $X$.\n",
    "\n",
    "1. Collect all the possible outcomes that give rise to event $\\{ X = x\\}$, from original sample space.\n",
    "2. Add their probability to obtain $P_X(x)$\n",
    "\n",
    "### Properties of $PMF$\n",
    "\n",
    "1. Since random variable $X$ iterates over every possible value of $x$, the total sum of $PMF$ of $X$ will always be $1$.\n",
    "2.  $PMF$ of $X$ will always be.\n",
    "$$\n",
    "O \\leqslant P_X(x) \\leqslant 1\n",
    "$$\n",
    "because of non-negative and normalization axiom of probability.\n",
    "3. Function of a random variable is a also a random variable.\n",
    "$$\n",
    "Y = aX + b\n",
    "$$\n",
    "Since value of $Y$ indirectly depends on outcome of experiment (ie. value of $X$). $Y$ is also a random variable.\n",
    "\n",
    "## Expectation / Expected Value (Mean)\n",
    "\n",
    "The $PMF$ of a random variable $X$ provides us with several numbers, the probability of all the possible values of $X$. Using this we can compute the average (weighted average) or the **expected value** of $X$ when we do large number of experiment. This weighted average of the values of $X$ is called the expectation of $X$ and is calculated by,\n",
    "\n",
    "$$\n",
    "E[X] = \\sum x.p_X(x)\n",
    "$$\n",
    "\n",
    "It is the average of values of \"$x$\" weighted with its probability.\n",
    "\n",
    "For example, consider a game where you take the minimum of some amount on every turn. So we calculate the expected value of random variable as follow.\n",
    "\n",
    "In this case our random variable is a function which return the minimum amount for a turn. So we can assume a $PMF$ as follows.\n",
    "\n",
    "$$\n",
    "P_X(k) = \\begin{cases} \n",
    "\\frac{1}{5} & \\text{Probability of wining 2€} \\\\\n",
    "\\frac{3}{2} & \\text{Probability of wining 3€} \\\\\n",
    "\\frac{1}{5} & \\text{Probability of wining 4€}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "E[X] = 2 * \\frac{1}{5} + 3 * \\frac{3}{2} + 4 * \\frac{1}{5} = 3.7\\text{€}\n",
    "$$\n",
    "\n",
    "Thus $3.7$€ is the average income if played for a long time.\n",
    "\n",
    "### Property of Expectation.\n",
    "\n",
    "Let $X$ be a random variable and let,\n",
    "\n",
    "$$\n",
    "Y = aX + b\n",
    "$$\n",
    "then,\n",
    "\n",
    "$$\n",
    "E[Y] = aE[X] + b\n",
    "$$\n",
    "\n",
    "Because adding a constant \"$b$\" to every value will only add \"$b$\" to the average. And similarly multiplying \"$a$\" constant to every value will only multiple \"$a$\" to the average.\n",
    "\n",
    "### Expectation of a function of random variable.\n",
    "\n",
    "Let $X$ be a random variable and $Y$ be a function of the random variable $Y = g(X)$, then \n",
    "\n",
    "$$\n",
    "E[Y] = \\sum_{x} g(X) P_X(x)\n",
    "$$\n",
    "\n",
    "We can prove this preposition as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_i g(x_i)p(x_i) &= \\sum_j \\sum_{i:g(x_i)=y_j} g(x_i)p(x_i) \\\\\n",
    "&= \\sum_j \\sum_{i:g(x_i)=y_j} y_jp(x_i) \\tag{Y = g(x)} \\\\\n",
    "&= \\sum_j y_j \\sum_{i:g(x_i)=y_j} p(x_i) \\\\\n",
    "&= \\sum_j y_j P\\{g(X) = y_j\\} \\\\\n",
    "&= E[g(X)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Another way to look at it as follow,\n",
    "\n",
    "![Expected value of function](/static/img/notes/mathematics/intro-probability/function_expected_value.png)\n",
    "\n",
    "Which means when calculating $E[Y]$ we can treat $E[X]$ as new new sample space.\n",
    "\n",
    "## Variance\n",
    "\n",
    "Another property of $PMF$ is variance, or how far the values are spread from the mean of the $PMF$. We could find the average spread of data from mean by following formula.\n",
    "\n",
    "$$\n",
    "E[X - E[X]]\n",
    "$$\n",
    "\n",
    "where $E[X]$ is our mean and $X$ are all values of random variable and we take average of all of them.\n",
    "\n",
    "But our difference could be both positive or negative depending on weather it is to right or left of our mean. And adding one positive and one negative will tamper our average value. To solve this we take square of the distance. We also cloud have taken absolute value of the difference but squaring also has one more advantage, it gives more weight to values farther away from mean. So new formula becomes,\n",
    "\n",
    "$$\n",
    "E[(X - E[X])^2]\n",
    "$$\n",
    "\n",
    "Thus formula for variance of $X$ is denoted by $Var(X)$ is given by.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Var(X) &= E[(X - E[X])^2] \\\\\n",
    "&\\text{OR} \\\\\n",
    "Var(X) &= E[X^2] - E[X]^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Properties of Variance.\n",
    "\n",
    "1. $Var(X) \\geqslant 0$, as square areas are positive.\n",
    "2. $Var(aX + b) = a^2 Var(X)$, because adding \"$b$\" only sifts values to either side but doesn't changes spread. and multiplication increases the spread.\n",
    "\n",
    "## Standard Deviation\n",
    "\n",
    "Another measure of dispersion is standard deviation of $X$,, which is defined as square root of the variance denoted by \n",
    "\n",
    "$$\n",
    "\\sigma X = \\sqrt{Var(X)}\n",
    "$$ \n",
    "\n",
    "The standard deviation is easier to interpret because it has same unit as $X$. For example if $X$ is in meters, variance will be in meters<sup>2</sup> but standard deviation will be in meters.\n",
    "\n",
    "## Example of Random variables.\n",
    "\n",
    "### Bernoulli Random Variable\n",
    "\n",
    "The Bernoulli random variable takes two values $1$ or $0$. For example.\n",
    "\n",
    "$$\n",
    "X = \\begin{cases}\n",
    "1 & \\text{if x is head} \\\\\n",
    "2 & \\text{if x is tail}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "It's $PMF$ is \n",
    "\n",
    "$$\n",
    "P_X(x) = \\begin{cases}\n",
    "P & \\text{if x = 1} \\\\\n",
    "1-P & \\text{if x = 0}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $P =$ probability of getting heads.\n",
    "\n",
    "### Binomial Random Variable\n",
    "\n",
    "A coin is tossed $n$ times. At each toss, the coin has a probability of $P$ of getting heads. Then $X$ be the number of heads in the $n$-toss. Then binomial random variable is given as.\n",
    "\n",
    "$$\n",
    "P_X(k) = P(X = k) = \\binom{n}{k} p^k(1 - p)^{n - k}  \\; \\;\\; k=0,1,2,...,n\n",
    "$$\n",
    "\n",
    "#### Properties of Binomial Random Variable.\n",
    "\n",
    "Suppose we have a binomial random variable with parameters $n$ and $p$. We can calculate the expected value as such.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[X^k] = \\sum\\limits_{i=0}{n} i^k \\binom{n}{i} p^i(1 - p)^{n - i} \\\\\n",
    "&= \\sum\\limits_{i=1}{n} i^k \\binom{n}{i} p^i(1 - p)^{n - i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using the identity\n",
    "\n",
    "$$\n",
    "i\\binom{n}{i} = n \\binom{n-1}{i-1}\n",
    "$$\n",
    "\n",
    "gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[X^k] &= np \\sum\\limits_{i=1}{n} i^{k-1} \\binom{n-1}{i-1} p^{i-1}(1 -p)^{n-i} \\\\\n",
    "&= np \\sum\\limits_{j=0}{n-1}(j + 1)^{k -1}\\binom{n - 1}{j} p^j(1-p)^{n-1-j} \\tag{by letting j=i-1} \\\\\n",
    "&= npE[(Y + 1)^{k - 1}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $Y$ is a binomial random variable with parameters $n-1, p$. Setting $k = 1$ in the preceding equation yields.\n",
    "\n",
    "$$\n",
    "E[X] = np\n",
    "$$\n",
    "\n",
    "That is, the expected number of successes that occur in $n$ independent trials when each is a success with probability $p$ is equal to $np$. Setting $k=2$ on the preceding equation and using the preceding formula for the expected value of a binomial random variable yields\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[X^2] &= npE[Y + 1] \\\\\n",
    "&= np[(n - 1)p + 1]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since $E[X]=np$ we obtain\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Var(X) &= E[X^2] - (E[X])^2 \\\\\n",
    "&= np[(n - 1)p + 1] - (np)^2 \\\\\n",
    "&= np(1 -p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Summing up, we have shown the following:\n",
    "\n",
    "If $X$ is a binomial random variable with parameters $n$ and $p$, then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[X] &= np \\\\\n",
    "Var(X) &= np(1 - p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Poisson Random Variable\n",
    "\n",
    "A random variable $X$ takes on one of the following values $0, 1, 2$ is said to be a Poisson random variable with parameter $\\lambda$ if, for some $\\lambda > 0$,\n",
    "\n",
    "$$\n",
    "p(i) = P\\{ X = i \\} = e^{-\\lambda}\\frac{\\lambda^{i}}{i!} \\;\\;\\;\\; i = 0,1,2,...\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "note_info": {
   "description": "Basic probability axioms and Random Variables",
   "image": "static/img/notes/mathematics/intro-probability/logo.jpg",
   "slug": "intro-probability",
   "title": "Introduction To Probability"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "344px",
    "width": "555px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
